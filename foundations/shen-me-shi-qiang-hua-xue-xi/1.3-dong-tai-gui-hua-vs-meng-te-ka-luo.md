# 1.3-动态规划 VS 蒙特卡洛

### 动态规划

#### 动态规划引入

我们看了一通过使用奖励喝惩罚来训练机器人，举个例子，假设给扫地机器人从房间回到厨房的任务，也就是自己的充电的地方，机器人可以采用向前、向后、向左、向右、旋转的动作，回到充电站奖励100，如果在路上撞击到任何的东西，则给出惩罚-10。

有一种方法叫做动态规划，由理查德 贝勒曼（Richard Bellman） 于1957年发明，动态规划可以更好地**称为目标定位**，因为它通过把**复杂的高级问题分解成越来越小的子问题**来解决，**直到它得到一个简单的子问题**，这个子问题可以不需要更多的信息来解决。

机器人可以先将问题分解为“待在这个房间里”与“离开这个房间”，而不是试图想出一长串原始动作将自己带到达终点。“**因为它有房子的完整地图，它知道它需要离开房间，因为码头在厨房里。然而它仍然不知道什么样的动作序列会允许它离开房间**，所以它将问题分解为“走向门”或“离开门”。因为门离码头更近，而且从门到厨房有一条路径，机器人知道它需要向门移动，但它同样不知道原始动作的顺序将使它朝门移动。最后，它需要决定是向左、向右、向右还是反向移动。它能看到门在它前面，所以它向前移动。它将保持这一过程，直到它离开房间，当它必须做一些目标分解，直到它到达厨房。

{% hint style="info" %}
这就是动态规划的本质。它是解决某些类型问题的通用方法，这些问题可以被分解为子问题和子问题，它在生物信息学、经济学和计算机科学等许多领域都有应用。
{% endhint %}

#### 动态规划的局限性

为了应用Bellman的动态规划，我们必须能够将问题分解成我们知道如何求解的子问题。但即使这个看似无伤大雅的假设在现实世界中也很难实现。例如：如何将自动驾驶汽车的“从a点到达B点而不会发生碰撞”这一高级目标，分解成不会发生碰撞的子问题?孩子学走路是先解决简单的子步行（sub-walking）问题吗?在RL中，我们经常遇到一些微妙的情况，其中可能包含一些随机元素，我们不能像Bellman所说的那样应用动态规划。事实上，DP可以被认为是一系列问题解决技术的一个极端，另一个极端是随机试验和误差。

{% hint style="info" %}
动态规划适用于可以将问题简化成多个子问题的任务，显然，RL问题动态规划不适用
{% endhint %}

另一种看待这种连续学习的方式是，在某些情况下，我们对环境的知识最多，而在另一些情况下，我们对环境的知识最少，我们需要在每种情况下采用不同的策略。如果你需要在自己家里上厕所，你确切地知道\(好吧，至少无意识地知道\)什么样的肌肉运动序列会让你从任何起始位置上厕所。这是因为你非常了解你的房子——在你的脑海中有一个或多或少完美的房子模型。如果你去参加一个你从未去过的房子的派对，你可能不得不四处寻找，直到你自己找到洗手间\(即试错\)，因为你没有一个关于这个人房子的好的模型。

#### 蒙特卡洛的引入

试错策略通常属于蒙特卡罗方法的范畴。蒙特卡罗方法**本质上是来自环境的随机抽样**。 在许多现实问题,我们至少有一些关于环境如何运行的知识,所以我们最终采用 由一些试验和错误以及一些探索（我们已经了解环境）可以直接很容易解决子目标的混合策略。

#### 混合策略的理解

一个混合策略的简单例子是，如果你在房子里一个未知的位置被蒙住眼睛，让你通过扔鹅卵石和倾听噪音来找到卫生间。你可以从把高级目标\(找到洗手间\)分解成更容易实现的子目标开始: 弄清楚你现在在哪个房间。 为了解决这个次级目标，你可以向随机方向扔几颗小石子，评估房间的大小，这可能会给你足够的信息，让你推断出你所在的房间——比如卧室。然后你就需要转向另一个子目标: 摸索到门，这样你才能进入走廊。然后你又开始扔石子，但因为你记得上次随机扔石子的结果，所以你可以将你的石子投向不太确定的区域。重复这个过程，你可能最终找到你的浴室。在这种情况下，您将同时应用动态规划的目标分解和蒙特卡罗方法的随机抽样。











