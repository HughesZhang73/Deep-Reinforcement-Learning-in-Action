# 1.6-如何理解深度强化学习

我们已经提出了强化学习的理由，但为什么是深度强化学习呢? RL早在深度学习普及之前就存在了。事实上，一些最早的方法\(出于学习的目的，我们将着眼于此方法\)仅仅涉及将经验存储在查找表中\(例如Python字典\)并在算法的每次迭代中更新该表。这个想法是让代理在环境中运行，看看发生了什么，并将它的经历存储在某种数据库中。一段时间后，你可以回顾这个知识数据库，观察哪些有用，哪些没用。没有神经网络或其他奇特的算法。

对于非常简单的环境，这实际上非常有效。例如，在Tic- 有255,168个有效的界面位置。查找表\(也称为内存表\)将有这么多条目，它们从每个状态映射到特定的操作 \(如图1.12所示\)和观察到的奖励\(未描述\)。在训练过程中，该算法可以学习哪种移动会导致更有利的位置，并在记忆表中更新该条目。

![1.12](../../.gitbook/assets/image%20%2839%29.png)

{% hint style="info" %}
一字棋的动作查找表，只有三个条目，“玩家”\(这里指算法\)在这里玩 当玩家获得一个棋盘位置时，查找表将决定他们下一步的行动。游戏中每个可能的状态都有一个入口
{% endhint %}

一旦环境变得更加复杂，使用内存表就变成了内部表。例如，电子游戏的每个屏幕配置都可以被认为是不同的状态，想象一下在电子游戏中存储有效像素值的所有可能组合!DeepMind的DQN算法在玩雅达利游戏时，每一步被输入4张84×84的灰度图像，这将导致256^{28228}种独特的游戏状态\(每个像素点有256种不同的灰度，以及 4 \* _84 \*_ 84 = 28228 个像素\)。这个数字比可观测宇宙中原子的数量要大得多，而且肯定装不下计算机内存。这是在图像被缩小到原来210×160像素的彩色图像之后。

{% hint style="info" %}
一系列三帧的突破。球的位置在每一帧略有不同。如果您使用的是一个查找表，那么这将等同于在表中存储三个唯一的条目。查找表是不实用的，因为有太多的游戏状态要存储。
{% endhint %}

存储所有可能的状态是不可能的，但我们可以尝试限制这种可能性。在”打砖块“游戏中，你可以控制屏幕底部的一个桨，它可以向左或向右移动; 游戏的目标是使球偏转并尽可能打到位于屏幕顶部可能多的块，在这种情况下，我们可以定义约束——只看球返回球拍时的状态，因为当我们在屏幕顶部等待球时，我们的动作并不重要。或者我们可以提供我们自己的特征——而不是提供原始图像，只提供球、桨和剩余块的位置。然而，**这些方法要求程序员理解游戏的潜在策略，它们不能推广到其他环境中**。

这就是深度学习的用武之地。**深度学习算法可以学习抽象出像素具体排列的细节，并可以学习一种状态的重要特征**。由于深度学习算法具有有限数量的参数，我们可以使用它将任何可能的状态压缩成我们可以有效处理的状态，然后使用新的表示来做出我们的决定。由于使用神经网络,雅达利DQN只有1792参数\(具有16 个 8×8 过滤器的神经网络, 32 个 4×4过滤器,和256个节点的全连接隐藏\)而不是将需要256 28228键/值对整个状态空间来存储。 

在打砖块游戏中，深度神经网络可能会自己学习去识别 必须程序员手工设计的查找表方法具有的相同高级特性。也就是说，它可以学习如何 “看” 球、球拍、木块以及识别球的方向。考虑到它只提供原始像素数据，这是相当惊人的。更有趣的是，学到的高级功能可以转移到其他游戏或环境中。

深度学习是使RL最近取得成功的秘密武器。没有其他类型的算法已经证明了深层神经网络的表示能力，效率和灵活性。此外，神经网络实际上相当简单!





