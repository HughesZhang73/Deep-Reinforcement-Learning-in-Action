# 1.5-强化学习可以用途？

监督学习等无法实现通用的人工智能（artificial general intelligence \(AGI\)），我们最终寻找的是一种通用的学习机器，它可以在很少甚至没有监督的情况下应用于多个问题，并且其技能可以跨领域转移。大型数据丰富的公司可以从有监督的方法中充分受益，但较小的公司和组织可能没有资源来利用机器学习的力量。通用学习算法将为每个人创造公平的竞争环境，而强化学习是目前对这类算法最有希望的方法。

RL的研究和应用仍处于成熟阶段，但近年来已经有了许多令人兴奋的发展。谷歌的DeepMind研究集团展示了一些令人印象深刻的结果，并获得了国际关注。第一次是在2013年，当时的算法可以在超人的水平上玩一系列雅达利游戏。 以前试图创造代理来解决这些游戏的尝试包括**对潜在算法进行微调，以理解游戏的具体规则**，这通常被称为特征工程（feature engineering）。

这些功能工程方法适用于特定游戏，但它们无法将任何知识或技能转移到新游戏或领域。 DeepMind的深度q -网络\(DQN\)算法足够稳健，可以工作在七款游戏上\(见图1.11\)。它只有屏幕上的原始像素作为输入，只是被告知要最大化分数，但算法却学会了如何超越专业的人类水平。

![](../../.gitbook/assets/image%20%2841%29.png)

{% hint style="info" %}
DeepMind的DQN算法成功地学会了如何玩7款雅达利游戏，只把原始像素作为输入，把玩家的分数作为目标来最大化。以前的算法，如IBM的“深蓝”\(Deep Blue\)，需要经过微调才能玩特定的游戏。

在超过10 170个合法的棋盘位置上，暴力算法\(IBM的深蓝\(Deep Blue\)用来在国际象棋中获胜\)是不可行的。AlphaGo之所以能够做到这一点，很大程度上是因为它玩了数百万次模拟的围棋游戏，并学习了哪些动作可以最大限度地获得玩好游戏的奖励。与雅达利的案例类似，AlphaGo只能获得与人类棋手相同的信息:棋子在棋盘上的位置。
{% endhint %}

虽然算法可以比人类更好地玩游戏，但RL的前景和潜力远远超过制作更好的游戏机器人。DeepMind能够创建一个模型，将谷歌的数据中心冷却成本降低40%，这是我们在本章前面探讨过的一个例子。自动驾驶汽车使用RL来学习哪些动作\(加速、转弯、刹车、信号\)可以让乘客准时到达目的地，并学习如何避免事故。研究人员正在训练机器人完成任务，如学习跑步，而无需明确编程复杂的运动技能。

很多例子都是高风险的，比如开车。你不能让学习型机器通过反复试验来学习如何开车。幸运的是，让学习机在无害环境模拟器中发挥作用的成功例子越来越多。一旦他们掌握了模拟器，让他们在现实世界中尝试真正的环境。我们将在本书中探讨的一个例子是算法交易。 在所有的股票交易中，有很大一部分是由计算机执行的，几乎没有人工操作人员的输入。这些算法交易员大多被管理着数十亿美元资产的大型对冲基金操纵。然而，在过去的几年里，我们看到越来越多的个体交易者对建立交易算法感兴趣。的确, _Quanticopian_ 提供了一个平台，个人用户可以在Python中编写交易算法，并在一个安全的模拟环境中测试它们。如果算法表现良好，它们就可以用来交易真钱。许多交易员通过简单的启发式和基于规则的算法取得了相对的成功。然而，股票市场是动态的和不可预测的，因此持续学习的RL算法的优势是能够实时适应不断变化的市场条件。

在本书的开头，我们要解决的一个实际问题是广告投放。 许多网络企业从广告中获得大量收入，而广告收入通常与这些广告获得的点击量有关。将广告投放到能够最大化点击量的地方是一个很大的动机。然而，做到这一点的唯一方法是利用用户的知识来展示最合适的广告。我们通常不知道用户的哪些特征与正确的广告选择有关，但我们可以使用RL技术取得一些进展。如果我们给RL算法一些可能有用的信息用户\(我们称之为环境,或状态的环境\),告诉它最大化广告点击,它将学习如何把数据输入到它的目标,它最终会从中学到最点击广告会产生一个特定的用户。







