# 1.4-强化学习框架

Richard Bellman将动态规划作为解决某些控制或决策问题的一般方法，但它占据了研究的 RL连续问题的一个极端。RL框架本质上是一组核心术语和概念，每个RL问题都可以用它们来表达。这不仅为其他工程师和研究人员提供了一个标准化的语言,这也迫使我们制定我们的问题的方式是适合动态编程问题分解,这样我们可以在当地迭代优化子问题和实现全局高层目标取得有效进展。同时，它也相当简单容易理解。

#### 如何理解框架

举个例子：大型服务器机房或者数据中心需要冷却散热系统，普遍的做法是花很多钱安装专业的冷却系统。例如谷歌的数据中心，还有一种平民的做法是安装空调，并且一直开着。但是服务器没有满载的时候发热是很小的，不需要过多的冷却，所以一直开着空调会很浪费电，专业的冷却系统价格也很高，成本会非常的大，并且假设现有的冷却系统没有机器学习学修算法加持优化。

**\(1\) 目标**

RL框架的第一步是**确定总体目标，**在这种情况下，我们的总体目标是在我们中心的服务器不能超过某个阈值温度的约束下，最小化冷却花费的资金。虽然这似乎是两个目标，但我们可以将它们捆绑成一个新的复合目标函数。这个函数返回一个误差值，指出在给定服务器的当前成本和温度数据的情况下，我们在满足这两个目标方面的偏离程度。我们的目标函数返回的实际数字并不重要;我们只是想让它尽可能的低。因此，我们需要我们的RL算法最小化这个目标\(误差\)函数相对于某些输入数据的返回值，这些输入数据肯定包括运行成本和温度数据，但也可能包括其他有助于算法预测数据中心使用情况的有用上下文信息。

**\(2\)环境**

通常，RL（或控制）任务的环境（environment）是任何动态过程，其产生与实现我们的目标相关的数据。尽管我们使用“环境”作为技术术语，对于日常生活中的我们来说，这个不算很抽象。试想，我们假设自己作为一种高级RL算法的实例，我们始终处在某种生活环境中，我们的眼睛和耳朵会不断处理环境所产生的信息，因此我们才可以正常生活。由于环境是一个动态过程（dynamic process）（一种时间的函数），因此它可能会产生大小和类型不同的连续数据流。为使算法更友好，我们需要**获取此环境数据并将其捆绑为离散的数据包，我们将其称为状态（环境），然后在每个离散时间步长将其提供给我们的算法**。状态反映了我们在特定时间的环境知识，就像数码相机在某个时间捕获场景的离散快照（并生成一致格式的图像）一样。

{% hint style="info" %}
到目前为止，总而言之，我们定义了一个目标函数（通过优化温度来最小化成本），该函数是环境（数据中心和任何相关过程）的状态（当前成本，当前温度数据）的函数。我们模型的最后一部分是RL算法本身。这可以是任何参数算法，可以通过修改参数来从数据中学习以最小化或最大化某些目标函数。它并不需要是深度学习算法; RL是一个独立的领域，与任何特定学习算法无关。
{% endhint %}

**\(3\)行动**

在之前的几节，我们讨论了监督学习和强化学习的区别，强化学习（或者一般说的控制任务）需要在控制人物中算法需要进行决策和执行动作，这些动作将对将来发生的事情产生因果关系，“**采取行动**”在RL框架中是一个关键词，它的意思或多或少与你期望的意思相同。然而，**所采取的每一个行动都是对当前环境状态的分析，并试图根据这些信息做出最佳决策的结果**。

**\(4\)奖励**

RL框架的最后一个概念是，在每个动作被执行之后，算法会得到一个奖励。奖励是学习算法在实现全局目标时表现如何的一个\(局部\)信号。奖励可以是积极的信号\(游戏邦注:如表现好，继续执行动作\)，也可以是消极的信号\(如不要这么做\)，尽管我们将这两种情况都称为“奖励”。奖励信号是学习算法在更新自身以期望在下一个环境状态中表现更好时必须遵循的唯一线索。

{% hint style="info" %}
在我们的上面的机房和数据中心的例子中，只要算法的操作减少了误差值，我们就可以向算法授予+10\(任意值\)奖励。或者更合理地说，我们可以根据误差的减少程度来给予相应的奖励。如果它增加了误差，我们就会给它一个负奖励。
{% endhint %}

**\(5\)代理**

代理（agent**）**是任何RL问题的行动或决策学习算法。 我们可以将以上5个组合在一起，如图1.8所示

![1.8](../../.gitbook/assets/image%20%2836%29.png)

{% hint style="info" %}
RL算法的标准框架。代理在环境中执行一个操作，比如移动棋子，然后更新环境的状态。它采取的每一个行动都会得到奖赏 \(例如，赢得比赛+1，输了比赛-1，否则0\)。RL算法以长期最大化回报为目标重复这一过程，并最终学习环境如何工作。
{% endhint %}

{% hint style="info" %}
在我们的例子中，我们希望agent可以学会降低冷却的费用，除非我们向agent提供关于机房或者数据中心完整环境信息和知识，否则agent需要进行一定程度的试错并产生误差，如果我们幸运的话，agent可能会学习得非常好，可以在不同的环境中使用，而不是在最初机房的环境中。由于代理是学习者，它被实现为某种学习算法。由于这是一本关于深度强化学习的书，我们的代理将使用深度学习算法\(也称为深度神经网络，见图1.9\)来实现。但请记住，RL更多的是关于问题和解决方案的类型，而不是任何特定的学习算法，你当然可以使用深度神经网络的替代方案。事实上，在本章的末尾我们会使用神经网络来代替实现
{% endhint %}

![1.9](../../.gitbook/assets/image%20%2838%29.png)

{% hint style="success" %}
输入数据\(即环境在某一时刻的状态\)被输入到代理\(在本书中实现为深度神经网络\)，然后代理对数据进行评估，以便采取行动。这个过程比这里显示的要复杂一些，但这就抓住了本质。
{% endhint %}

代理人的唯一目标是在长期内最大化其预期回报。它只是重复这个循环:处理状态信息，决定采取什么行动，看看它是否获得奖励，观察新的状态，采取另一个行动，等等。如果我们正确地设置了所有这些，代理最终将学会理解它的环境，并在每一步做出可靠的正确决策。这种通用机制可以应用于自动车辆、聊天机器人、机器人、自动股票交易、医疗保健等等。我们将在下一节和整个课程中探讨其中的一些应用程序.

{% hint style="warning" %}
你在这本书中的大部分时间将用来学习如何在我们的标准模型中构造问题，以及如何实现足够强大的学习算法去解决困难的问题。对于这些示例，不需要构造环境，可以使用现有环境\(如游戏引擎或其他api\)。例如，OpenAI已经发布了一个Python Gym库，它为我们提供了许多环境和一个直观的界面，以便我们的学习算法进行交互。图1.10的代码显示了设置和使用其中一种环境是多么简单——赛车游戏只需要五行代码。
{% endhint %}

![1.10](../../.gitbook/assets/image%20%2840%29.png)

```text
import gym
env = gym.make('CarRacing-v0')
env.reset()
env.step(action)
env.render()
```

{% hint style="info" %}
OpenAI Python库提供了许多环境和易于使用的界面，用于与学习算法进行交互。仅用几行代码，就加载了一款赛车游戏。
{% endhint %}





