# 章节总结

* **强化学习**是机器学习的一个子类。RL算法通过在某些环境中最大化奖励来学习，当涉及到做决定或采取行动的问题时，它们是有用的。RL算法原则上可以使用任何统计学习模型，但是使用深度神经网络已经变得越来越流行和有效。
* **代理**是任何RL问题的焦点。它是RL算法的一部分，用于处理输入以决定采取哪个操作。在这本书中，我们主要关注作为深度神经网络实现的代理。
* **环境**是代理在其中操作的潜在动态条件。更一般地说，环境是为代理生成输入数据的任何流程。例如，我们可能有一个特工在飞行模拟器中驾驶飞机，所以模拟器就是环境。
* **状态是环境的快照**，代理可以访问该环境并使用该环境做出决策。环境通常是一组不断变化的条件，但我们可以从环境中采样，这些样本在特定的时间是我们给agent的环境状态信息。
* 一个**行为**是由一个行为主体做出的一个决定，它在其所处的环境中产生了变化。移动一个特定的棋子是一种行动，摁下汽车的油门也是一种行动。
* 奖励是环境在代理人采取行动后给予其积极或消极的信号。奖励是施动者得到的唯一学习信号。RL算法\(即agent\)的目标是使奖励最大化。
* RL算法的通用管道是一个循环的代理接收输入数据\(的环境\),代理评估数据和需要一个行动从一组可能的行为由于其当前状态,行动改变了环境,和环境然后发送奖励信号和代理的新状态信息。然后循环往复。**当agent被实现为深度神经网络时，每次迭代都基于奖励信号评估一个损失函数，并反向传播以提高agent的性能**。





