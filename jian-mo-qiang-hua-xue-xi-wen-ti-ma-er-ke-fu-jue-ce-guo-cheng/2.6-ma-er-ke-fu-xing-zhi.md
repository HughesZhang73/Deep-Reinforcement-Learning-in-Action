# 2.6-马尔可夫性质

在我们的情境盗贼问题中，我们的神经网络引导我们在不参考任何其他先验状态的情况下选择给定状态的最佳行动。我们只是给它当前的状态，它就会为每个可能的行动产生预期的奖励。这是强化学习中一个重要的性质叫做马尔可夫性质。一个具有马尔可夫特性的游戏\(或任何其他控制任务\)被称为马尔可夫决策过程\(MDP\)。在MDP中，当前状态本身就包含了足够的信息，可以让你选择最优的行动来最大化未来的回报。将控制任务建模为 MDP是强化学习中的一个关键概念。

MDP模型极大地简化了RL问题，因为我们不需要考虑所有以前的状态或行为——我们不需要内存，我们只需要分析当前的情况。因此，我们总是试图将一个问题建模为\(至少近似地\)一个马尔科夫决策过程。21点纸牌游戏 \(也被称为21\)是一个MDP，因为我们可以通过知道我们当前的状态\(我们有什么牌，庄家的一张牌是正面朝上的\)来成功地玩游戏。

为了测试你对马尔可夫属性的理解，考虑以下列表中的每个控制问题或决策任务，看看它是否具有马尔可夫属性:

* 驾驶汽车 
* 决定是否投资于股票 
* 选择病人的治疗 
* 诊断病人的疾病
* 预测哪个队会在足球比赛中获胜 
* 选择最短的路线\(距离\)一些目的地 
* 瞄准枪射杀一个遥远的目标

好吧，让我们看看你做得怎么样。以下是我们的答案和简要解释:

* 开车通常被认为具有马尔可夫属性，因为你不需要知道10分钟前发生了什么，就可以最佳地驾驶你的车。你只需要知道现在一切都在哪里，你想去哪里。 
* 决定是否投资于股票不符MDP的 因为你想知道股票过去的表现以便做出决策。
* 选择一种治疗方法似乎具有马尔可夫特性，因为你不需要了解一个人的生平，就可以选择一种好的治疗方法来治疗目前困扰他们的疾病。 
* 相比之下,诊断\(而不是治疗\)肯定会需要知识——过去的边缘状态。为了作出诊断，了解病人症状的历史进程往往是非常重要的。 
* 预测足球队获胜没有马尔可夫性质,因为,像股票的例子中,你需要知道足球队的过去的表现做出很好的预测
* 选择到目的地的最短路径具有马尔可夫属性因为你只需要知道不同路径到目的地的距离，它不依赖于昨天发生的事情。 
* 瞄准枪射杀一个遥远的目标还具有马尔可夫性质,因为所有你需要知道目标在哪里,也许当前条件下风速和细节你的枪。你不需要知道昨天的风速。

我们希望你们能理解在那些例子中你们可以论证它是否具有马尔可夫性质。例如，在诊断一个病人时，你可能需要知道他们最近的症状历史，但如果这是记录在他们的医疗记录中，我们认为完整的医疗记录是我们的当前状态，那么我们就有效地诱发了马尔可夫特性。记住这一点很重要:许多问题可能天生就不具有马尔可夫特性，但我们通常可以通过向状态中插入更多信息来诱发它。

DeepMind的深度q-learning\(或称深度q -网络\)算法仅从原始像素数据和当前得分就学会了玩雅达利游戏。雅达利游戏有马尔可夫属性吗?不完全是。在游戏《吃豆人》中，如果我们的状态是来自当前帧的原始像素数据，我们就不知道几个贴图之外的敌人是正在接近我们还是正在移动远离我们，这将极大地影响我们的行动选择。这就是为什么DeepMind的执行实际上是在输入游戏的最后四帧，有效地将非MDP变为MDP。在最后四帧，代理可以访问所有玩家的方向和速度。

图2.8给出了一个轻松的Markov决策过程示例，使用了我们到目前为止讨论过的所有概念。你可以看到有一个三元素的状态空间S = {哭宝宝，睡宝宝，笑宝宝}，两元素动作空间a = {喂食，不喂食}。此外，我们还记录了转移概率，即从一个行动到结果状态概率的映射\(我们将在下一节中再次讨论这一点\)。当然，在现实生活中，你作为代理人并不知道转移概率是多少。如果你这样做了，你就会有一个环境的模型。稍后您将了解到，有时代理可以访问环境的模型，有时则不能。在代理不能访问模型的情况下，我们可能希望我们的代理学习环境的模型\(它可能只是接近真实的底层模型\)。

![&#x56FE;2.8](../.gitbook/assets/image%20%2871%29.png)

具有三个状态和两个动作的简化MDP图。在这里，我们为照顾婴儿的父母决策过程建模。如果婴儿哭了，我们可以选择喂他或不喂他，有可能婴儿会进入一种新的状态，我们就会得到-1、+1或+2的奖励 \(基于宝宝的满意度\)。







