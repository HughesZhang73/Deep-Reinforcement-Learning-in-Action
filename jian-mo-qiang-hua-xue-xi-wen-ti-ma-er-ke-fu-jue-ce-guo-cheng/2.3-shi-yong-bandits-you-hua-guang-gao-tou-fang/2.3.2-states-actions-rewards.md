# 2.3.2-States, actions, rewards

在我们继续之前，让我们巩固一下到目前为止我们介绍过的一些术语和概念。强化学习算法试图以计算机能够理解和计算的方式模拟世界。特别是,RL算法模型世界如果它仅仅涉及一组状态 state: _**S**_ \(state space\), 这是一组关于环境的特性,一系列的可以在一个给定的状态中的动作 action:**A** \(action space\)和 在一个特定的状态下采取的动作所得到的奖励 reward r. 当我们谈及在特定的状态下采取特定的动作时,我们经常称之为 状态动作对 state-action pair \(s, a\)

{% hint style="info" %}
任何RL算法的目标都是在整个episode 过程中最大化奖励。
{% endhint %}

由于之前的N-armed bandit 问题并没有状态空间,只有动作空间,我们只需要去学习动作和奖励之间的关系就行.我们通过使用一个查找表来存储我们从特定行动中获得奖励的经验来了解这种关系。我们储存了行动奖励配对\(a k, r_k\)，其中在进行第k项活动时获得的奖励是与采取动作 a\_k_ 相关的所有过去活动的平均值。

在我们的n-armed bandit问题中，我们只有10个动作，所以10行的查找表是非常合理的。但是当我们引入一个带有上下文 bandit 的状态空间时，我们开始得到一个可能的状态-行为-奖励元组的组合爆炸。 例如，如果我们有一个包含100个状态的状态空间，并且每个状态与10个操作相关联，那么我们就需要存储和重新计算1000个不同的数据片段。在我们将在本书中考虑的大多数问题中，状态空间都非常大，因此简单的查找表是不可行的

这就是深度学习的用武之地。经过适当的训练后，神经网络能够很好地学习抽象概念，摆脱那些没有价值的细节。它们可以从数据中学习可组合的模式和规律，从而在有效压缩大量数据的同时保留重要信息。因此，神经网络可以用来学习状态-行动对和奖励之间的复杂关系，而我们不必将所有这些经验作为原始记忆存储。我们通常把RL算法中基于某些信息做出决策的部分称为agent。为了解决我们已经讨论过的上下文强盗，我们将使用一个神经网络作为我们的代理。

首先，我们将花一点时间介绍PyTorch，这是我们将在整本书中使用的构建神经网络的深度学习框架。



