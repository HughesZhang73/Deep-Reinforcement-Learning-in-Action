# 2.5-解决contextual bandits

我们为contextual bandit创建了一个模拟环境。模拟器包括状态\(0到9代表网络中10个网站中的1个\)、奖励生成\(广告点击\)和选择动作的方法\(10个广告中的哪一个要提供服务\)。下面的清单显示了context bandit环境的代码，但是不要花太多时间考虑它，因为我们想演示如何使用它，而不是如何编写它。

```text
# coding=utf-8
import numpy as np
import random


class ContextBandit:
    def __init__(self, arms=10):
        self.arms = arms
        self.init_distribution(arms)
        self.update_states()
        self.state = 0
    
    def init_distribution(self, arms):
        """
        states 的数量等于杠杆的数量，让事情变得简单。每一行代表一个状态，每一列代表一个臂。
        :param arms:
        :return:
        """
        self.bandit_matrix = np.random.rand(arms, arms)
    
    def reward(self, prob):
        reward = 0
        for i in range(self.arms):
            if random.random() < prob:
                reward += 1
        return reward
    
    def get_state(self):
        return self.state
    
    def update_states(self):
        self.state = np.random.randint(0, self.arms)
        
    def get_reward(self, arm):
        return self.reward(self.bandit_matrix[self.get_state()][arm])
    
    def choose_arm(self, arm):
        """
        选择一个拉杆，返回一个奖励以及更新状态
        :param arm:
        :return:
        """
        reward = self.get_reward(arm)
        self.update_states()
        return reward

```

下面的代码片段演示了如何使用该环境。我们需要构建的唯一部分是代理，这通常是任何RL问题的症结所在，因为构建环境通常只涉及使用一些数据源设置输入/输出或插入现有API。

```text
    
if __name__ == '__main__':
    env = ContextBandit(arms=10)
    state = env.get_state()
    reward = env.choose_arm(1)
    print(state)
    print(reward)
```

模拟器由一个简单的Python类ContextBandit类组成，可以初始化为特定数量的手臂。为简单起见，状态的数目等于手臂的数目，但一般来说状态空间往往比动作空间大得多。这个类有两个方法:一个是get\_state\(\)，调用它时不带参数，它将返回从均匀分布中随机抽样的状态。 在大多数问题中，你的状态将来自一个更加复杂的分布。调用另一个方法choose\_arm\(…\)，将模拟投放广告，并返回奖励\(例如，与广告点击次数成比例\)。我们需要总是调用get\_state，然后按这个顺序调用choose\_arm，以不断地获取新的数据来学习。

ContextBandit模块还包括一些辅助函数，如softmax函数和一个单热编码器。单热编码向量是除1个元素外所有元素都设置为0的向量。唯一的非零元素被设置为1，并指示状态空间中的特定状态。

与在n个行动中使用单一静态奖励概率分布\(如我们最初的强盗问题\)不同，情境强盗模拟器为每个状态的行动设置了不同的奖励分布。也就是说，对于每一种状态，我们将有n种不同的softmax奖励分配。因此，我们需要了解状态和它们各自的奖励分配之间的关系，然后了解哪个行动在给定状态下的概率最高。

与本书中所有的项目一样，我们将使用PyTorch来构建神经网络。在这种情况下，我们将建立一个两层前馈神经网络，使用矫正线性单元\(ReLU\)作为激活函数。神经网络第一层接受10个元素的one-hot\(也称为1-of-K，其中除1个元素外所有元素都为0\)编码了状态向量，最后一层返回一个10个元素的向量，表示给定状态下每个操作的预期奖励。

图2.6显示了我们所描述的算法的前向传递。与查找表方法不同，我们的神经网络代理将学会预测给定状态下每个动作将产生的奖励。然后我们使用softmax函数给出动作的概率分布，并从这个分布中抽样选择一个arm\(广告\)。选择一只手臂会给我们一个奖励，我们会用它来训练我们的神经网络。

![](../.gitbook/assets/image%20%2868%29.png)

{% hint style="info" %}
一个计算图的一个简单的10武装上下文强盗问题。get\_state\(\)函数返回一个状态值，该值被转换为 one-hot 向量，成为一个双层神经网络的输入数据。神经网络的输出是每个可能行动的预测奖励，这是一个密集的向量，通过softmax从行动的结果概率分布中采样行动。选定的操作将返回奖励并更新环境的状态。θ1和θ2表示每层的权重参数。这个**N**, **R**，和**P**符号分别表示自然数（0，1，2，3，…）、实数（浮点数）和概率。上标表示向量的长度，所以ℙ^10表示10个元素向量，其中每个元素都是一个概率（所有元素的总和为1）
{% endhint %}

最初，我们的神经网络将产生一个随机向量，如状态0时 有\[1.4, 50, 4.3, 0.31, 0.43, 11, 121, 90, 8.9, 1.1\]，我们将在这个向量上运行softmax并采样一个动作，很可能是动作6\(从动作0到9\)，因为这是示例向量中最大的数字。选择行动6将会产生8的奖励。然后我们训练我们的神经网络产生向量\[1.4,50,4.3,0.31, 0.43, 11, 8, 90, 8.9, 1.1\]，因为这是我们从行动6中获得的真正奖励，其余值保持不变。下一次当神经网络看到状态0时，它会对接近8的行为6产生奖励预测。当我们在许多状态和行为上不断地这样做时，神经网络最终将学会对给定状态下的每个行为预测准确的奖励。因此，我们的算法将能够每次选择最佳行动，最大化我们的奖励。

```text
    arms = 10
    
    # 设定超参数，参数来指定模型的结构
    N, D_in, H, D_out = 1, arms, 100, arms
    """
    N: batch 大小
    D_in: 输入维度
    H: 隐藏维度
    D_out: 输出维度
    """
    
    model = torch.nn.Sequential(
        # 神经网络第一层
        torch.nn.Linear(D_in, H),
        torch.nn.ReLU(),
        
        # 神经网络第二层
        torch.nn.Linear(H, D_out),
        torch.nn.ReLU(),
    )
    
    # 我们将在这里使用均方误差损失函数，但其他函数也可以
    
    loss_fn = torch.nn.MSELoss()
    
    # 现在，我们通过实例化ContextBandit类来设置一个新环境，并向其构造函数提供武器的数量。
    # 记住，我们已经在类中建立了一个环境使得武器的数量等于状态的数量
    env = ContextBandit(arms)
```

该算法的主要for循环将非常类似于我们最初的n武装的bandit算法，但我们添加了运行神经网络和使用输出选择动作的步骤。我们将定义一个名为train的函数\(如清单2.10所示\)，该函数接受我们前面创建的环境实例、我们想要训练的时代数和学习速率。

在函数中，我们将为当前状态设置一个PyTorch变量，我们需要使用one\_hot\(…\)编码函数对其进行一次性编码:

```text
def one_hot(N, pos, val=1):
    one_hot_vec = np.zeros(N)
    one_hot_vec[pos] = val
    return one_hot_vec
```

一旦我们进入主要的for循环训练，我们将使用随机初始化的当前状态向量运行我们的神经网络模型。它将返回一个向量，该向量表示它对每个可能操作的值的猜测。首先，由于模型没有经过训练，它会输出一堆随机值。

我们将在模型的输出上运行softmax函数，以生成操作的概率分布。然后，我们将使用环境中的choose\_arm\(…\)函数选择一个动作，该函数将返回执行该动作所生成的奖励;它还将更新环境的当前状态。我们要把奖励转回去 \(这是一个非负整数\)变成一个热向量，我们可以使用它作为我们的训练数据。

然后，根据我们给出的模型状态，我们将使用这个奖励向量运行一个反向传播步骤。由于我们使用神经网络模型作为我们的动作值函数，我们不再有任何形式的动作值数组来存储“记忆”;所有的东西都被编码到神经网络的权值参数中。整个train函数如下面的清单所示。

```text
def train(env, epochs=5000, learning_rate=1e-2):
    DrawImage.image_init()
    
    reward_hist = np.zeros(50)
    reward_hist[:] = 5
    runningMean = np.average(reward_hist)
    
    cur_state = torch.Tensor(one_hot(arms, env.get_state()))  # 获取到环境的状态，转换成Pytorch的变量
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
    rewards = []
    for i in range(epochs):
        y_pred = model(cur_state)  # 启动神经网络前向传播得到奖励预测
        av_softmax = softmax(y_pred.data.numpy(), tau=2.0)  # 用softmax将奖励预测转换为概率分布
        av_softmax /= av_softmax.sum()  # 对分布进行正态化，确保和为1
        chioce = np.random.choice(arms, p=av_softmax)  # 概览地选择新的行动
        cur_reward = env.choose_arm(chioce)  # 采取动作，接收奖励
        one_hot_reward = y_pred.data.numpy().copy()  # 转换Pytorch 张量数据到 numpy 数组
        one_hot_reward[chioce] = cur_reward  # 更新one_hot_reward数组作为标记的训练数据使用
        reward = torch.Tensor(one_hot_reward)
        rewards.append(cur_reward)
        loss = loss_fn(y_pred, reward)
        if i % 50 == 0:
            runningMean = np.average(reward_hist)
            reward_hist[:] = 0
            # plt.scatter(i, runningMean)
            DrawImage.image_draw(i, runningMean)
        reward_hist[i % 50] = cur_reward
        optimizer.zero_grad()
        loss.backward()  # Backward pass: compute gradient of the loss with respect to model parameters
        optimizer.step()  # Calling the step function on an Optimizer makes an update to its parameters
        cur_state = torch.Tensor(one_hot(arms, env.get_state()))  # Updates current environment state
    
    DrawImage.image_show()
    return np.array(rewards)
```

继续运行这个函数。当我们训练这个网络5000个epochs，我们可以绘制出在训练时间内获得的奖励的移动平均值。我们的神经网络确实能够很好地理解状态、行为和奖励之间的关系。任何游戏的最大奖励是10，我们的平均奖励是8.5，这接近于这个强盗的数学优化。我们的第一个深度强化学习算法工作!好吧，这不是一个非常深的网络，但仍然有效!

![](../.gitbook/assets/image%20%2865%29.png)

{% hint style="info" %}
以上是我复现书中的思路,代码经过重构,根据书中的思路补全了绘图代码.一下是原书中结果图2.7
{% endhint %}

![&#x56FE;2.7](../.gitbook/assets/image%20%2870%29.png)

{% hint style="info" %}
使用两层神经网络作为动作-价值函数的训练图，显示玩情境模拟游戏的平均奖励。我们可以看到平均奖励在训练期间迅速增加，这表明我们的神经网络在成功地学习。
{% endhint %}

这里贴出全部源代码,可以完美运行

```text
# coding=utf-8
# 建立神经网络。是一个前馈网络，有两层

import numpy as np
import torch
import random
import matplotlib.pyplot as plt


class ContextBandit:
    def __init__(self, arms=10):
        self.arms = arms
        self.bandit_matrix = []
        self.state = 0
        self.init_distribution(arms)
        self.update_states()
    
    def init_distribution(self, arms):
        """
        states 的数量等于杠杆的数量，让事情变得简单。每一行代表一个状态，每一列代表一个臂。
        :param arms:
        :return:
        """
        self.bandit_matrix = np.random.rand(arms, arms)
    
    def reward(self, prob):
        reward = 0
        for i in range(self.arms):
            if random.random() < prob:
                reward += 1
        return reward
    
    def get_state(self):
        return self.state
    
    def update_states(self):
        self.state = np.random.randint(0, self.arms)
    
    def get_reward(self, arm):
        return self.reward(self.bandit_matrix[self.get_state()][arm])
    
    def choose_arm(self, arm):
        """
        选择一个拉杆，返回一个奖励以及更新状态
        :param arm:
        :return:
        """
        reward = self.get_reward(arm)
        self.update_states()
        return reward


# 绘图重新封装成类
class DrawImage(object):
    
    fig, ax = plt.subplots(figsize=(9, 6))
    
    @classmethod
    def image_init(cls):
        plt.figure(12)
        cls.ax.set_xlabel("Plays", fontsize=15)
        cls.ax.set_ylabel("Mean Reward", fontsize=15)
    
    @classmethod
    def image_draw(cls, data1, data2):
        cls.ax.scatter(data1, data2, marker="o", alpha=0.7)
        
    @classmethod
    def image_show(cls):
        plt.show()
    

def one_hot(N, pos, val=1):
    one_hot_vec = np.zeros(N)
    one_hot_vec[pos] = val
    return one_hot_vec


def softmax(av, tau=1.12):
    n = len(av)
    probs = np.zeros(n)
    for i in range(n):
        softm = (np.exp(av[i] / tau) / np.sum(np.exp(av[:] / tau)))
        probs[i] = softm
    return probs


def train(env, epochs=5000, learning_rate=1e-2):
    DrawImage.image_init()
    
    reward_hist = np.zeros(50)
    reward_hist[:] = 5
    runningMean = np.average(reward_hist)
    
    cur_state = torch.Tensor(one_hot(arms, env.get_state()))  # 获取到环境的状态，转换成Pytorch的变量
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
    rewards = []
    for i in range(epochs):
        y_pred = model(cur_state)  # 启动神经网络前向传播得到奖励预测
        av_softmax = softmax(y_pred.data.numpy(), tau=2.0)  # 用softmax将奖励预测转换为概率分布
        av_softmax /= av_softmax.sum()  # 对分布进行正态化，确保和为1
        chioce = np.random.choice(arms, p=av_softmax)  # 概览地选择新的行动
        cur_reward = env.choose_arm(chioce)  # 采取动作，接收奖励
        one_hot_reward = y_pred.data.numpy().copy()  # 转换Pytorch 张量数据到 numpy 数组
        one_hot_reward[chioce] = cur_reward  # 更新one_hot_reward数组作为标记的训练数据使用
        reward = torch.Tensor(one_hot_reward)
        rewards.append(cur_reward)
        loss = loss_fn(y_pred, reward)
        if i % 50 == 0:
            runningMean = np.average(reward_hist)
            reward_hist[:] = 0
            # plt.scatter(i, runningMean)
            DrawImage.image_draw(i, runningMean)
        reward_hist[i % 50] = cur_reward
        optimizer.zero_grad()
        loss.backward()  # Backward pass: compute gradient of the loss with respect to model parameters
        optimizer.step()  # Calling the step function on an Optimizer makes an update to its parameters
        cur_state = torch.Tensor(one_hot(arms, env.get_state()))  # Updates current environment state
    
    DrawImage.image_show()
    return np.array(rewards)


if __name__ == '__main__':
    
    arms = 10
    
    # 设定超参数，参数来指定模型的结构
    N, D_in, H, D_out = 1, arms, 100, arms
    """
    N: batch 大小
    D_in: 输入维度
    H: 隐藏维度
    D_out: 输出维度
    """
    
    model = torch.nn.Sequential(
        # 神经网络第一层
        torch.nn.Linear(D_in, H),
        torch.nn.ReLU(),
        
        # 神经网络第二层
        torch.nn.Linear(H, D_out),
        torch.nn.ReLU(),
    )
    
    # 我们将在这里使用均方误差损失函数，但其他函数也可以
    
    loss_fn = torch.nn.MSELoss()
    
    # 现在，我们通过实例化ContextBandit类来设置一个新环境，并向其构造函数提供武器的数量。
    # 记住，我们已经在类中建立了一个环境使得武器的数量等于状态的数量
    
    env = ContextBandit(arms)
    train(env)
```



