# 2.8-小结

* 状态空间是系统可能处于的所有可能状态的集合。在国际象棋中，这是所有有效棋盘配置的集合。动作是一个将状态s映射到新状态s '的函数。一个行为可能是随机的，比如它将一个状态s概率地映射到一个新的状态s '。在一组可能的新状态中可能存在某种概率分布。动作空间是特定状态的所有可能动作的集合。
* 环境是状态、行为和奖励的来源。如果我们构建一个RL算法来玩游戏，那么游戏就是环境。环境模型是状态空间、行动空间和转移概率的近似值。
* 奖励是环境产生的信号，表明在给定状态下采取行动的相对成功程度。期望报酬是一个统计概念，非正式地指一些随机变量X\(在我们的例子中是报酬\)的长期平均值，表示为E\[X\]。例如,在n-armed土匪问题中,E\[R \| a\] \(动作a 的期望奖励\)是采取每n项行动的长期平均回报。如果我们知道所有动作概率分布,然后我们可以计算  对于有N个玩家游戏中 期望奖励的精确值.其中:

![](../../.gitbook/assets/image%20%2867%29.png)

其中N是玩游戏的次数，p\_{i}是指行为a\_{i}的概率，r是指最大可能的奖励。

* 代理是一种RL算法，它学会在给定的环境中以最优的方式行事。代理通常被实现为深度神经网络。代理的目标是最大化期望回报，或等效地，以得到最高价值状态为目标。
* 策略是一种特殊的策略。形式上，它是一个函数，它要么接受一个状态，然后产生一个动作，要么在给定状态的情况下，在动作空间上产生一个概率分布。一个常见的策略是贪心策略，在这个策略中，在概率为ε的情况下，我们在动作空间中选择一个随机的动作，在概率为ε - 1的情况下，我们选择到目前为止已知的最好的动作。
* 一般来说，价值函数是在给定一些相关数据的情况下返回预期回报的函数。如果没有额外的上下文，它通常指的是一个状态值函数，这个函数接受一个状态，并返回在该状态开始并根据某些策略执行的预期回报。Q值是给定状态-行为对的期望奖励，Q函数是给定状态-行为对时产生Q值的函数.
* 马尔科夫决策过程是一种决策过程，通过它可以做出最好的决策，而不参考之前的历史状态。

