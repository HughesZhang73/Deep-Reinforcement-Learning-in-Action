# 2.7-预测未来的奖励:值\(Value\)和策略\(Policy\)方程

信不信由你，我们实际上在前面的章节中偷偷带进了很多知识。 我们建立n-armed bandit和contextual bandit解决方案的方式是标准的强化学习方法，因此，在我们所做的背后有一大堆既定的术语和数学。我们已经引入了一些术语，如状态和动作空间，但我们主要只是用自然语言描述事物。为了让您理解最新的RL研究论文，也为了让我们以后的章节少一些冗长，熟悉行话和数学是很重要的。

让我们回顾并确定到目前为止您所学的内容\(总结见图2.9\)。 强化学习算法本质上构造了一个agent，它在某些环境中发挥作用。环境通常是一种游戏，但更普遍地说，是任何产生状态、行动和奖励的过程。代理可以访问当前状态的环境,这是所有关于环境的数据在一个特定的时间点,s\_{t}∈ S_, 使用这种状态信息,代理需要一个_动作a\_{t} ∈ A,它可能确定性地或概率地改变环境到一个新的状态 s\_{t+1}

通过采取行动将一种状态映射到一种新状态的概率称为转移概率\(**transition probability**\)。代理收到一个奖励r\_{t}，因为它在状态st中采取了动作a\_{t}，导致了一个新的状态s\_{t +1}。我们知道agent\(我们的强化学习算法\)的最终目标是最大化它的回报。真正产生奖励的是状态转换\(s\_{t}→s\_{t +1}\)，而不是动作本身，因为动作可能会导致糟糕的状态。如果你是在一部动作电影中\(没有双关语\)，你从一个屋顶跳到另一个屋顶，你可能优雅地落在另一个屋顶上，或者完全错过它而掉下去——重要的是你的危险\(这两种可能的结果状态\)，而不是你跳下去的事实\(动作\)。

![&#x56FE;2.9](../../.gitbook/assets/image%20%2864%29.png)

{% hint style="info" %}
一种强化学习算法的一般过程。环境产生状态和回报。代理采取一个动作，t，给定一个状态，s\_{t}，在时间t，并收到一个奖励 r\_{t} 代理的目标是通过学习在给定的状态下采取最佳行动来获得最大的回报。
{% endhint %}



