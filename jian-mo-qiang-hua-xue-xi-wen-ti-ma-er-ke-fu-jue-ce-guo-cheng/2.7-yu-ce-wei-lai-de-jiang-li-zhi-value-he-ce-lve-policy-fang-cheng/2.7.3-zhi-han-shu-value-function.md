# 2.7.3-值函数\(Value function\)

值函数\(value function\)是将状态或状态-操作对映射到期望值\(expected value\)的函数 \(期望的回报\)处于某种状态或在某种状态下采取某种行动。你可能会从统计数据中想起，预期奖励只是处于某种状态或采取某些行动后获得的长期平均奖励。当我们说到值函数时，我们通常指的是状态值函数。

![&#x8868;2.7](../../.gitbook/assets/image%20%2869%29.png)

这是一个函数，它接受状态s，并返回在该状态开始并根据策略π采取行动的预期回报。价值函数依赖于策略的原因可能不是很明显。在我们的情境盗贼问题中，如果我们的策略是选择完全随机的行动\(即来自均匀分布的样本行动\)，那么状态的价值\(期望奖励\)可能会非常低，因为我们并没有选择最好的可能行动。相反，我们希望使用一种策略，它不是对动作的均匀分布，而是在 sample 时产生最大回报的概率分布。也就是说，策略决定了观察到的奖励，而价值函数是观察到的奖励的反映。

在我们的第一个n-武装强盗问题中，你被介绍到状态-动作-价值函数。这些函数通常被称为Q函数或Q值，这就是深度Q学习的来源，因为，正如你在下一章将看到的，深度学习算法可以被用作Q函数。

![&#x8868;2.8](../../.gitbook/assets/image%20%2866%29.png)

事实上，我们在某种程度上实现了一个深度Q网络来解决我们的上下文盗贼**contextual bandit**问题\(尽管它是一个相当浅的神经网络\)，因为它本质上是一个Q函数。我们对它进行训练，让它准确估计在给定状态下采取行动的预期回报。我们的策略函数是神经网络输出的softmax函数。

我们已经通过使用n-armed和contextual 强盗问题作为示例，介绍了强化学习中的许多基本概念。在本章中，我们还学习了深度强化学习。在下一章中，我们将实现一个成熟的deep Q-network，类似于DeepMind过去玩雅达利游戏的算法,达到超人的水平。这将是我们在这里所讨论的内容的自然延伸。

