# 2.2-解决多臂老虎机问题\(the multi-arm bandit\)

现在我们已经准备好开始一个真正的强化学习问题，并在我们进行的过程中查看解决这个问题所需的相关概念和技能。但是，在我们构建像AlphaGo这样的东西之前，让我们先考虑一个简单的问题。假设你在一家赌场，你面前有10台老虎机，上面挂着一个醒目的牌子，上面写着“免费游戏!”最大支付是10美元!“哇，不错!”很好奇，你问其中一名员工发生了什么，因为事情好得令人难以置信，她说， “这是真的，你可以尽情地玩，这是免费的。每台老虎机保证会给你0到10美元的奖励。对了，别告诉别人，但那些 10台老虎机每个都有不同的平均奖励，所以试着找出哪台平均奖励最多，你就会赚到很多钱!”

这是什么赌场?管他呢，我们先想想怎么才能赚最多的钱!对了，有个笑话:老虎机的另一个名字是什么?一个独臂强盗!明白了吗?它有一只手臂\(杠杆\)，它通常偷你的钱。我们可以称这种情况为10人武装的强盗问题，或者更普遍地称其为n人武装的强盗问题，其中n是老虎机的数量。到目前为止，这个问题听起来很奇怪，稍后您将看到这些n-武装强盗\(或多武装强盗\)问题确实有一些非常实际的应用。

让我们更正式地重申一下我们的问题。我们有n个可能的动作\(这里n = 10\)，其中一个动作意味着拉动一个特定老虎机的手臂或杠杆，在这个游戏的每一次\(k\)中，我们可以选择拉动一个杠杆。在采取行动后 \(a\)我们将获得一个奖励，R\_{k}\(在k点的奖励\)。每个杠杆都有一个唯一的奖励\(奖励\)概率分布。ps: R\_{k} 指的是如下,由于gitbook 中公式不好表示

$$
R_{k}
$$

每个杠杆都有一个唯一的支付\(奖励\)概率分布。例如，如果我们有10台老虎机并玩了许多游戏，那么老虎机3号的平均奖励是9美元，而老虎机1号的平均奖励只有4美元。当然，因为每次游戏的奖励都是有概率的，所以杠杆1有可能会给我们一个奖励 每场9美元。但如果我们玩了很多游戏，我们就会发现老虎机的平均数量奖励第1个比第3个低。

我们的策略应该是玩几次游戏，选择不同的杠杆，观察每个行动的奖励。然后我们只选择平均回报最大的杠杆。因此，我们需要一个基于我们之前的行动\(a\)的预期奖励概念。我们将这一预期奖励称为Q\_{k}\(a\):你给函数一个操作\(假设我们在进行k\)，它将返回执行该操作的预期奖励。这在这里正式显示:

![](../../.gitbook/assets/image%20%2851%29.png)

也就是说，行为a在游戏k中所期望的奖励是我们之前因为采取行为a而获得的所有奖励的算术平均值。因此，我们之前的行为和观察会影响我们未来的行为。我们甚至会说，我们之前的一些行为强化了我们现在和未来的行为，但我们稍后会回到这一点。这个函数 q\_{k} \(a\)被称为值函数\(value function\),因为它告诉我们某物的值。特别地，它是一个行动价值函数\(action-value function\)，因为它告诉我们采取特定行动的价值。 由于我们通常用符号Q来表示这个函数，它也经常被称为Q函数。稍后我们将回到值函数并给出更复杂的定义，但现在这就足够了。















