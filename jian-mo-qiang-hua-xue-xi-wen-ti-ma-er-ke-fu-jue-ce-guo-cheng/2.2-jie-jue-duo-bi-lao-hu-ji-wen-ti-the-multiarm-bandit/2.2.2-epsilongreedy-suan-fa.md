# 2.2.2 Epsilon-greedy 算法

我们需要探索其他杠杆\(其他老虎机\)来发现真正的最佳行动。对之前算法的一个简单修改是将它改为ε \(epsilon\)-贪婪算法，我们将随机选择一个动作a，剩下的时间\(概率1 -ε\)我们将根据我们目前已知的过去的操作选择最好的杠杆。大多数时候我们会贪心，但有时我们也会冒险，选择随机的杠杆来看看会发生什么。结果当然会影响我们未来的贪婪行为。用以下代码进行解决：

```text
    n = 10  # 老虎机的数量
    record = np.zeros((n, 2))
    probs = np.random.rand(n)  # 和每个杠杆相关联的隐藏概率
    eps = 0.2  # 贪心策略影响动作选择的参数 epsilon
```

在这个赌场的例子中，我们将解决一个有10个武装的强盗问题，所以n = 10。我们还定义了一个长度为n的numpy数组，其中充满了可以理解为概率的随机浮点数。probs数组中的每个位置都对应一个arm，这是一个可能的动作。例如，第一个元素的索引位置为0，所以动作0是arm 0。每只手臂都有一个相关的概率来衡量它能得到多少奖励。

我们为每只手臂选择的奖励概率分布是这样的:每只手臂都有一个概率，例如0.7，最大奖励是10美元。 我们将设置一个达到10的for循环，在每一步，如果随机浮点数小于arm的概率，它将给奖励加1。因此，在第一次循环中，它构成了一个随机浮点数\(例如，0.4\)。0.4小于0.7，所以奖励+= 1。在下一次迭代中，它将构成另一个随机浮动值\(游戏邦注:如0.6\)，这也小于0.7，所以奖励+= 1。 这将持续到我们完成10次迭代，然后我们返回最终的总奖励，可以是0到10之间的任何值。手臂概率是0.7，这样做到无穷大的平均奖励是7，但在任何一次游戏中它都可能是或多或少。使用以下代码定义奖励函数：

```text
def get_reward(prob, n=10):
    reward = 0
    for i in range(n):
        if random.random() < prob:
            reward += 1
    return reward
```

下图中显示使用概览为0.7 时 多臂强盗 问题奖励的分布：  


![](../../.gitbook/assets/image%20%2860%29.png)

接下来需要进行定义的是找到最佳手臂的贪婪策略，我们需要一种保持跟踪哪一个杠杆被拉动以及得到的奖励是什么的方法。简单的，我们需要一个列表来存储观察（observation）例如\(arm, reward\), 比如\(2, 9\).代表我们选择了2号拉杆收到了的奖励值为9.这个列表随着游戏的进行会变得很长

这个方法确实很简单，可是，由于我们需要保持对每个拉杆平均奖励的追踪，即我们不需要存储每一次的观察（observation）。计算均值的一个数字列表,x_i\( 包含 i\),我们只需把所有的x\_i 的_值进行相加,然后除以x的数量, 用表示k。均值通常用希腊字母 μ 进行表示。

![](../../.gitbook/assets/image%20%2857%29.png)

希腊大写符号σ\(σ\)被用来表示一种求和运算。下面的 i 表示我们对每个元素\( x\_i \)求和。它基本上相当于for循环的数学等价:

如果我们已经有一个特定拉杆的平均奖励μ，我们可以更新这个平均值，当我们得到一个新的奖励通过重新计算平均值。我们基本上需要还原平均值，然后重新计算。为了撤销它，我们用μ乘以值的总数k。当然，这只给出了和，而不是原始的一组值——因为不能对相加的得到的和进行加数的还原。但是总数是我们需要用一个新值重新计算平均值。我们只是把这个和加上新的值然后除以k + 1，新的总值。

![](../../.gitbook/assets/image%20%2855%29.png)

当我们收集新数据时，我们可以使用这个方程来不断更新每个拉杆上观察到的平均奖励，这样我们只需要跟踪每个臂上的两个数字: **k\(观察到的值的数量\)** 和 **μ\(当前运行的平均值\)**。 我们可以很容易地将它存储在一个10×2 numpy数组中\(假设我们有10条臂\)。我们称这个数组为 **record**

数组的第一列将存储每个拉杆被拉动的次数，第二列将存储运行的平均奖励。让我们编写一个用于更新记录的函数，并给出一个新的操作和奖励。

```text
def update_record(record, action, r):
    new_r = (record[action, 0] * record[action, 1] + r) / (record[action, 0] + 1)
    record[action, 0] += 1
    record[action, 1] = new_r
    return record
```

这个函数接受 **record** 数组、一个 action \(这是arm的索引值\)和一个新的奖励观察。为了更新平均奖励，它只需要执行我们之前描述的数学函数，然后增加计数器，记录拉杆被拉动的次数。

接下来我们需要一个函数来选择要拉哪个拉杆。我们希望它选择与最高平均回报相关的拉杆，所以我们需要做的就是找到 **record** 数组中第1列中值最大的行。使用numpy的内置argmax函数可以很容易地做到这一点，该函数接受数组，找到数组中的最大值，并返回其下标位置。

```text
def get_best_arm(record):
    arm_index = np.argmax(record[:, 1], axis=0)  # 在记录数组的 第 1 列 上使用numpy argmax
    return arm_index
```

现在我们可以进入玩n-armed bandit游戏的主循环了。如果一个随机数大于epsilon参数，我们只需使用get\_best\_arm函数计算最佳操作并采取该操作。否则我们便会采取一些随机行动去确保一些探索。在选择手臂之后，我们使用get\_reward函数并观察奖励值。然后，我们用这个新的观察值更新 **record** 数组。拥有最高回报概率的拉杆最终会被选择得最多，因为它将会提供最高的平均回报。

在下面的列表中，我们将其设置为播放500次，并显示播放的平均奖励的matplotlib散点图。希望我们能够看到随着我们玩游戏次数的增加，平均奖励也会增加。

以下是解决多臂强盗问题的全部代码以及可视化代码：

```text
# coding=utf-8

import numpy as np
from scipy import stats
import random
import matplotlib.pyplot as plt


# class N_Armed_Bandit():

def get_best_action(actions):
    best_action = 0
    max_action_value = 0
    for i in range(len(actions)):  # 循环遍历所有可能的动作
        cur_action_value = get_action_value(actions[i])  # 获取当前动作的的值
        if cur_action_value > max_action_value:
            best_action = i
            max_action_value = cur_action_value
    return best_action


def get_reward(prob, n=10):
    reward = 0
    for i in range(n):
        if random.random() < prob:
            reward += 1
    return reward


def update_record(record, action, r):
    new_r = (record[action, 0] * record[action, 1] + r) / (record[action, 0] + 1)
    record[action, 0] += 1
    record[action, 1] = new_r
    return record


def get_best_arm(record):
    arm_index = np.argmax(record[:, 1], axis=0)  # 在记录数组的 第 1 列 上使用numpy argmax
    return arm_index


def draw_imag(data):
    
    plt.figure(12)
    fig, ax = plt.subplots(figsize=(9, 6))
    ax.scatter(np.arange(len(data)), data, marker='v', alpha=0.3)
    ax.set_xlabel("Plays", fontsize=15)
    ax.set_ylabel("Avg Reward", fontsize=15)
    ax.set_title('Solving the n-armed bandit')
    plt.show()

    
def draw_imags(data):
    plt.subplot(2, 2, 1)
    plt.scatter(np.arange(len(data)), data, marker='o', c='blue', alpha=0.1)
    
    plt.subplot(2, 2, 2)
    plt.scatter(np.arange(len(data)), data, marker='v', c='green', alpha=0.1)

    plt.subplot(2, 1, 2)
    plt.scatter(np.arange(len(data)), data, marker='*', c='red', alpha=0.3)
    plt.ylabel("Avg Reward", fontsize=10)
    plt.xlabel("Plays")
    plt.title('Solving the n-armed bandit')
    plt.show()

    
if __name__ == '__main__':
  
    n = 10  # 老虎机的数量
    record = np.zeros((n, 2))
    probs = np.random.rand(n)  # 和每个杠杆相关联的隐藏概率
    eps = 0.2  # 贪心策略影响动作选择的参数 epsilon
    rewards = [0]
    episdes = 500
    
    for i in range(episdes):
        if random.random() > eps:
            choice = get_best_arm(record)
        else:
            choice = np.random.randint(10)
        r = get_reward(probs[choice])
        
        record = update_record(record, choice, r)
        
        mean_reward = ((i + 1) * rewards[-1] + r) / (i + 2)
        rewards.append(mean_reward)
    
    draw_imags(rewards)
    # print(rewards)

```

代码运行结果如下所示：

![](../../.gitbook/assets/image%20%2856%29.png)

{% hint style="info" %}
从图中可以看出，玩了很多次游戏后，平均奖励确实有所提高。 我们的算法是学习的。然而这是一个如此简单的算法。原书结果如下所示：
{% endhint %}

![Figure 2.3 This plot shows that the average reward for each slot machine play increases over time, indicating we are successfully learning how to solve the n-armed bandit problem](../../.gitbook/assets/image%20%2852%29.png)

这里我们考虑的问题是一个平稳（stationary）问题因为拉杆的潜在回报概率分布不会随着时间改变。我们当然可以考虑这个问题的一个变体，当它不成立时——一个非平稳（nonstationary）问题。在这种情况下，一个简单的修改是允许新的奖励观察以一种倾斜的方式更新存储在记录中的平均奖励值，这样它将是一个加权平均值，朝着最新的观察值加权。这样，如果事情随着时间发生变化，我们就能在某种程度上追踪它们。我们不会在这里实现这个稍微复杂一点的变量，但是我们会在书的后面遇到非平稳的问题。







