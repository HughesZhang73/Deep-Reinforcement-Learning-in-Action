# 2.2.3-Softmax选择策略

想象一下另一种类型的多臂强盗问题:一位专门治疗心脏病患者的新医生。她有10种治疗选择，她看到的每个病人只能选择1种治疗。由于某些原因，她只知道这10种治疗方法对心脏病发作有不同的疗效和风险，她还不知道哪一种是最好的。我们可以使用前面解中的n武装的强盗算法，但我们可能需要重新考虑随机选择处理的ε-greedy策略。在这个新问题中，随机选择一种治疗方法可能会导致病人死亡，而不仅仅是损失金钱。我们真的很想确保我们不会选择最糟糕的治疗方法，但我们仍然希望有能力探索我们的选择以找到最好的治疗方法。

这时候使用softmax选项可能是最合适的。与在探索过程中随机选择行动不同，softmax为我们的选择提供了概率分布。概率最大的选项将等同于前一个解决方案中的最佳拉杆动作，但它也会给我们一些关于第二和第三个最佳动作的概念。这样我们就可以随机选择探索其他选项，同时避免最糟糕的选项，因为它们的概率很小，甚至是0。以下是softmax方程式:

![](../../.gitbook/assets/image%20%2853%29.png)

Pr\(A\)是一个函数，它接受动作值向量\(数组\)，并返回动作的概率分布，这样，高值动作的概率就更高。例如，如果你的 action-value 数组有四个可能的动作，它们目前都有相同的值，比如A =\[10,10,10,10\]，那么Pr\(A\) = \[0.25, 0.25, 0.25, 0.25\]。换句话说，所有的概率都是一样的，并且总和必须为1。

分数的分子对动作值数组除以参数τ取幂，生成与输入相同大小\(即长度\)的向量。分母相加除以每一个个体行为值的幂除以τ，得到一个单独的数字。

τ是一个称为温度（temperature）的参数，它缩放了行为的概率分布。 高温将导致概率非常相似，而低温将夸大行动之间的概率差异。为这个参数选择一个值需要有根据的猜测和一些试验和错误。数学指数 e^{x} 是numpy中对np.exp\(…\)的函数调用。这使得函数按元素遍历输入向量。Python中实现softmax函数如下所示：

```text
def softmax(av, tau=1.12):
    softm = np.exp(av / tau) / np.sum(np.exp(av / tau))
    return softm
```

当我们使用softmax实现之前的10个武装强盗问题时，我们不再需要get\_best\_arm函数了。由于softmax在我们可能的行动中产生一个加权概率分布，我们可以根据它们的相对概率随机选择行动。也就是说，我们的最佳行动将被更频繁地选择，因为它将拥有最高的softmax概率，但其他行动将以较低的频率被选择。

为了实现这一点，我们需要做的就是在记录数组的第二列\(列从1开始索引\)上应用**softmax**函数，因为这一列存储了每个操作的当前平均奖励\(动作值\)。它会把这些行为值转化为概率。然后我们用np.random.choice函数,它接受一个任意的输入数组x ,和一个参数p, 概览数组的每个元素和x中每一个元素相关联。因为我们的记录被初始化为0,softmax起初将返回一个均匀分布在多有的拉杆上,但这很快就会斜向分布, 无论动作与最高的奖励时候存在联系。以下是使用softmax和随机选择函数的例子:

```text
def sample():
    x = np.arange(10)
    av = np.zeros(10)
    p = softmax(av)
    print(x)
    print(av)
    print(p)
    ans = np.random.choice(x, p=p)
    print(ans)
```

结果:

![](../../.gitbook/assets/image%20%2861%29.png)

我们使用numpy range函数创建了一个从0到9的数组，对应于每个arm的索引，因此随机选择函数将根据提供的概率向量返回arm索引。我们可以使用与之前相同的训练循环;我们只需要改变拉杆选择部分，让它使用softmax而不是get\_best\_arm，我们还需要去掉作为贪心策略一部分的随机动作选择。

```text
if __name__ == "__main__":

    n = 10  # 老虎机的数量
    record = np.zeros((n, 2))
    probs = np.random.rand(n)  # 和每个杠杆相关联的隐藏概率
    eps = 0.2  # 贪心策略影响动作选择的参数 epsilon
    rewards = [0]
    episdes = 500

    for i in range(episdes):
        # if random.random() > eps:
        #     choice = get_best_arm(record)
        # else:
        #     choice = np.random.randint(10)
        
        p = softmax(record[:, 1])  # 计算每个拉杆相对于其当前动作值的softmax概率
        choice = np.random.choice(np.arange(n), p=p)  # 随机选择一只拉杆，但以softmax概率加权
        
        r = get_reward(probs[choice])
        record = update_record(record, choice, r)
        mean_reward = ((i + 1) * rewards[-1] + r) / (i + 2)
        rewards.append(mean_reward)
    draw_imags(rewards)
```

在这个问题上，Softmax操作选择似乎比epsilon-greedy方法做得更好，如图所示;它似乎更快地趋同于最优政策。softmax的缺点是必须手动选择τ参数。这里的Softmax对τ非常敏感，它需要一些时间来找到一个好的值。显然，对于贪心，我们必须设置参数，但是选择那个参数更直观。

![](../../.gitbook/assets/image%20%2858%29.png)

原图2.4 如下所示:

![Figure 2.4 With the softmax policy, the n-armed bandit algorithm tends to converge faster on the maximal mean reward](../../.gitbook/assets/image%20%2854%29.png)











