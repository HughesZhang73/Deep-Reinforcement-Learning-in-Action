# 学习选择最佳策略:策略梯度法

本章包含以下内容：

* 使用神经网络实现策略函数
* 介绍OpenAI Gym API
* 使用 REINFORCE 算法解决OpenAI的 Cartpole 问题

在前一章中，我们讨论了DQN，一种用神经网络近似Q函数的off-policy算法。Q-network的输出为给定状态下每个动作对应的Q值\(图4.1\);回想一下，Q值是奖励的期望\(加权平均值\)。

![&#x56FE;4.1 ](.gitbook/assets/image%20%28100%29.png)

{% hint style="info" %}
Q-network接受一个状态，并为每个动作返回Q值\(动作值\)。我们可以使用这些行动价值来决定采取哪些行动。
{% endhint %}

从Q网络中得到这些预测的Q值，我们可以使用一些策略来选择要执行的动作。在上一章节中我们使用epsilon-greedy的方法，即 我们随机选择一个概率为ε的行为，在概率为1 - ε的情况下我们选择与Q值最高相关的行为 \(考虑到目前的经验，Q-network预测的行动是最好的\)。我们还可以遵循许多其他策略，比如在Q值上使用softmax层。



















