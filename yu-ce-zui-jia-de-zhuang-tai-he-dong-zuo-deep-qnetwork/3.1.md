# 3.1-Q方程

在本书中，我们将使用一个非常简单的Gridworld引擎，它包含在GitHub存储库中。你可以在[http://mng.bz/JzKp](http://mng.bz/JzKp) 第三章的文件夹下下载.

图3.1中描述的Gridworld游戏显示了我们将从Gridworld开始的简单版本;我们将逐步解决更困难的游戏变体。我们的最初目标是训练DRL代理在Gridworld板上导航以达到目标，每次都遵循最有效的路线。但是在我们深入讨论这个问题之前，让我们回顾一下前一章的关键术语和概念，我们将在这里继续使用它们

![&#x56FE;3.1](../.gitbook/assets/image%20%2876%29.png)

{% hint style="info" %}
这是一个简单的Gridworld游戏初始化。 代理\(A\)必须沿着最短路径到达目标贴图\(+\)，并避免掉进坑\(-\)
{% endhint %}

状态 state 是我们的代理接收到的信息，它用来决定采取什么行动。它可以是电子游戏的原始像素，自动驾驶汽车的传感器数据，或者在Gridworld中，代表网格上所有对象位置的张量。

这个策略，表示为π，是我们的代理在提供一个状态时所遵循的策略。例如，在21点游戏中，一个策略可能是看着我们的手\(状态\)，然后随机拿牌或停牌。尽管这将是一项糟糕的策略，但要强调的重要一点是，该策略决定了我们采取哪些行动。一个更好的策略是不断地出牌直到我们达到19点

奖励 **reward** 是我们的代理采取动作之后所得到的反馈, 同时我们会收到新的状态 state. 在象棋游戏中，我们可以在代理执行导致其他玩家将死的操作时给予+1奖励，而在代理执行导致对方将死的操作时给予-1奖励。其他状态都可以得到0的奖励，因为我们不知道代理是否会赢。

我们的代理基于策略 π 做出一系列的动作,并重复这个过程直到 episode 结束，因此我们会得到一系列的状态、动作和结果奖励。

![](../.gitbook/assets/image%20%2873%29.png)

我们把从开始状态 S\_{1} 开始的策略中得到的奖励的加权和称为那个状态的值，或者一个状态值。我们可以用值函数 value function 来表示 V\_π\(s\)，接受初始状态并返回预期的总回报。

![](../.gitbook/assets/image%20%2874%29.png)

系数w1, w2等等，是我们在把它们加起来之前应用到奖励上的权重。例如，我们通常希望近期的奖励比未来的奖励更重要。这个加权和是一个期望值,许多定量领域常见的统计,这是通常简明地表示_**E**_ \[R⏐π, s\],解读为“给定策略 π 和 起始状态s 下奖励的期望”. 类似地，有一个动作值函数 **action-value function**  Q\_{π}\(s,a\)，它接受状态 S 和动作 A，并返回给定状态下采取动作的值;换句话说就是 _**E**_ \[R⏐π,s, a\]。一些RL算法或实现会使用其中一种或其他.

重要的是，如果我们的算法是基于学习状态值  state value \(而不是行动值 action value \)，我们必须记住，一个状态的值完全取决于我们的策略 π。以21点纸牌游戏为例，如果我们处于总共有20张牌的状态，我们有两种可能的操作，拿牌或停牌，只有当我们的策略表明在停牌时我们有20点时，这种状态的值才会高。如果我们的策略是在我们有20点的时继续拿牌，我们很可能会失败并输掉比赛，所以这个状态的值会很低。换句话说，一种状态的值相当于该状态中所采取的最高动作的值。



















/

