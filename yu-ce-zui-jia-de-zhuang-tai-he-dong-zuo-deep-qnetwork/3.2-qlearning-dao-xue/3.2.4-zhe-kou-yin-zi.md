# 3.2.4-折扣因子

参数γ，折扣因子 ，是一个介于0和1之间的变量，它控制着我们的代理人在做决策时对未来奖励折现多少。让我们举一个简单的例子。我们的代理需要做出选择，是选择一个导致0奖励然后+1奖励的行为，还是选择一个导致+1奖励然后0奖励的行为\(见图3.4\)。

![&#x56FE;3.4](../../.gitbook/assets/image%20%2880%29.png)

{% hint style="info" %}
一个动作轨迹的例子，它会导致相同的总奖励，但可能会有不同的价值，因为较近期的奖励通常比遥远的奖励更有价值
{% endhint %}

之前，我们将轨迹的值定义为期望的回报。图3.4中的两种轨迹都提供了+1的整体奖励，那么算法应该更倾向于哪种顺序的行动呢?我们怎样才能打破僵局?如果折现因子γ小于1，我们对未来回报的折现要大于对当前回报的折现。在这个简单的例子中，尽管两种路径都能获得+1奖励，但动作b获得+1奖励的时间比动作a晚，因为我们在未来会进一步考虑行动，所以我们更偏向于动作a。我们将行动b中的+1奖励乘以一个小于1的权重因子，所以我们将奖励从+1降低到0.8，这样行动的选择就很清楚了。

折扣系数和RL一样，在现实生活中也会用到。假设有人现在给你100美元，或者一个月后给你110美元。大多数人更愿意现在就拿到钱，因为我们在某种程度上对未来进行了折现，这是有道理的，因为未来是不确定的\(如果给你钱的人在两周后去世了呢?\)在现实生活中，你的折扣率取决于一个月后，某人会给你多少钱，让你对选择它和现在得到100美元没有区别。如果你一个月只接受200美元而不是现在的100美元，那么你的折扣率就是100美元/ 200美元= 0.5\(每月\)。这就意味着有人会在两个月给你400美元你才选择这个选项, 即不去选择现在得到100美元,因为我们0.5折扣1个月,下个月再乘0.5, 0.5×0.5 = 0.25,100 = 0.25 x, x = 400。也许你会发现折现是指数形式。某物在t时刻的值具有折扣因子 γ \[0,1\)记为 γ^{t}。

折扣系数必须在0和1之间，我们不应该让它完全等于1，因为如果我们根本不打折扣，我们将不得不考虑无限远的未来奖励，这在实践中是不可能的。即使我们以0.99999折现，最终也会有一段时间我们不再考虑任何数据，因为它将被折现为0。

在Q学习中，我们面临同样的决策:在学习预测Q值时，我们在多大程度上考虑了未来的观察奖励?不幸的是，对于这个问题没有明确的答案，也没有设置任何我们可以控制的超参数。我们只需要摆弄一下这些旋钮，看看根据经验哪种方法最有效。

值得指出的是，大多数游戏都是回合式的 episodic ，这意味着玩家在游戏结束前有多种机会采取行动，而许多游戏，如象棋，除了输赢并不会自然地分配分数。因此，这些游戏中的奖励信号很少，这使得基于试错法的学习很难可靠地学习任何东西，因为这需要频繁地看到奖励。

在《Gridworld》中，我们将游戏设计成任何未获胜的移动都将获得-1奖励，获胜的移动将获得+10奖励，失败的移动将获得-10奖励。只有在游戏的最后一步算法才能判断 “啊哈!现在我明白了!“因为在《Gridworld》游戏的每一episode中，玩家都可以用相当少的步数获胜，所以奖励稀少的问题并不是太糟糕，但在其他游戏中，这是一个非常严重的问题，即使是最先进的强化学习算法也无法达到人类水平。解决这一问题的一种方法是，停止依赖于最大化期望奖励的目标，而是指导算法去寻找新颖性，通过新颖性它将了解周围的环境，这一点我们将在第8章中讨论。









