# 3.2.5-建立网络

让我们深入探讨一下如何为这个游戏构建我们的深度学习算法。回想一下，神经网络有一种特定的体系结构或网络拓扑。当你构建一个神经网络时，你必须决定它应该有多少层，每个层有多少参数\(层的“宽度”\)，以及层是如何连接的。Gridworld非常简单，我们不需要建造任何花哨的东西。我们可以使用典型的矫正线性激活单元\(ReLU\)，使用只有几层的相当简单的前馈神经网络。唯一需要更仔细考虑的部分是如何表示输入数据，以及如何表示输出层。

我们将首先介绍输出层。在我们讨论Q-learning时，我们说 Q函数是一个函数，它接受一些状态和一些动作，并计算状态-动作对Q\(s,a\)的值。这就是Q函数最初的定义 \(图3.5\)。正如我们在前一章中提到的，还有一个状态值函数，通常表示为Vπ\(s\)，它计算某种状态的值，假设你遵循一个明确的策略 π.

一般来说，我们想要使用Q函数因为它可以告诉我们在某种状态下采取行动的值，所以我们可以采取具有最高预测值的行动。但是，在给定状态下，单独计算每个可能操作的Q值是相当浪费的，即使Q函数最初是这样定义的。一个更有效的程序，和DeepMind实施的深度 Q  学习的实现,是重塑Q函数作为向量值函数 vector-valued function ,意味着: 它不是为单个状态-动作对计算并返回单个Q值，而是**计算给定某些状态下所有动作的Q值，并返回所有这些Q值的向量**。所以我们可以用Q\_{A} \(s\)来表示这个新版本的Q函数，其中下标A表示所有可能操作的集合\(图3.5\)。

![&#x56FE;3.5](../../.gitbook/assets/image%20%2882%29.png)

{% hint style="info" %}
原始的Q函数接受一个状态-操作对，并返回该状态-操作对的值:一个单独的数字。DeepMind使用了一个修改后的向量值Q函数，该函数接受一种状态，并返回状态-动作值的向量，给定输入状态的每个可能动作对应一个向量。向量值Q函数更有效，因为您只需要为所有操作计算一次函数。
{% endhint %}

现在很容易部署神经网络作为Q函数的Q\_{A} \(s\)版本;最后一层将生成一个Q值的输出向量——每个可能的操作对应一个Q值。在Gridworld中，只有四个可能的操作\(上、下、左、右\)，因此输出层将生成4维向量。然后，我们可以直接使用神经网络的输出来决定使用某些操作选择过程\(如简单的贪心方法或softmax选择策略\)采取什么操作。在这一章里,我们将使用epsilon-greedy方法DeepMind一样\(图3.6\),而不是使用一个静态的ε值像我们最后一章那样,我们将初始化一个较大的值\(例如,1,所以我们将完全随机选择的一个动作\),我们将慢慢减量,一定数量的迭代之后,ε值将在一些小值上平稳。这样，**我们将允许算法在一开始探索和学习很多东西，然后它将通过利用它所学到的东西来实现最大化的奖励。希望我们能设定一个递减的过程，这样它就不会被过度或不足地探索，但这必须通过经验来检验**。

![](../../.gitbook/assets/image%20%2885%29.png)

{% hint style="info" %}
在epsilon-greedy行动选择方法中,我们设置了ε的参数值,如 0.1,在这个概率下我们将随机选择一个行动\(完全忽略预测Q值\)或概率1 - ε= 0.9,我们将选择与最高预测的Q值相关的动作。另一种有用的技术是从一个高的 ε 值开始，比如1，然后在训练迭代过程中慢慢减小它。
{% endhint %}

我们已经弄清楚了输出层，现在来处理其余的部分。在本章中，我们将构建一个只有三层的网络，宽度分别为164\(输入层\)、150\(隐藏层\)和4\(输出层\)。欢迎并鼓励您添加更多的隐藏层或调整隐藏层的大小——您可能会在更深的网络中获得更好的结果。我们选择在这里实现一个相当浅的网络，这样你就可以用你自己的CPU来训练模型。

{% hint style="success" %}
MacBook Air 1.7 GHz英特尔酷睿i7, 8gb内存，只有几分钟的训练时间
{% endhint %}





