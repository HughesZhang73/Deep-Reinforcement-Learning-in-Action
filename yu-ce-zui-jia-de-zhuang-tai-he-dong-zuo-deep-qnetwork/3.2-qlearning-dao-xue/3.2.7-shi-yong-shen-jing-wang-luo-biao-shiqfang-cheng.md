# 3.2.7-使用神经网络表示Q方程

我们构建一个神经网络作为Q函数。如你所知，在这本书中，我们使用PyTorch来实现我们所有的深度学习模型，但如果你更喜欢另一个框架，如TensorFlow或MXNet，移植这些模型应该是相当简单的。 

图3.8显示了我们将要构建的模型的总体架构。图3.9以弦图的形式展示该网络以及在弦上的数据类型

![&#x56FE;3.8](../../.gitbook/assets/image%20%2876%29.png)

{% hint style="info" %}
我们将使用神经网络模型来玩网格世界。该模型有一个输入层，可以接受64个长度的游戏状态向量，一些隐藏层\(我们使用一个，但两个是通用的\)，以及一个输出层，它为每个动作生成一个4个长度的Q值向量，给定状态。
{% endhint %}

![&#x56FE;3.9](../../.gitbook/assets/image%20%2875%29.png)

{% hint style="info" %}
DQN的弦图。输入是一个64长度的布尔向量，输出是一个由Q值组成的4长度的实向量。
{% endhint %}

为了使用pytorch实现该网络,我们使用 nn 模块, 该模块式Pytorch 高级接口,类似于Tensorflow中的Keras

```text
    # Neural network Q function
    
    l1 = 64
    l2 = 150
    l3 = 100
    l4 = 4
    
    model = torch.nn.Sequential(
        torch.nn.Linear(l1, l2),
        torch.nn.ReLU(),
        torch.nn.Linear(l2, l3),
        torch.nn.ReLU(),
        torch.nn.Linear(l3, l4)
    )
   
    loss_fn = torch.nn.MSELoss()
    learing_rate = 1e-3
    optimizer = torch.optim.Adam(model.parameters(), lr=learing_rate)
    
    gamma = 0.9
    epsilon = 1.0
```

到目前为止，我们所做的就是建立神经网络模型，定义一个损失函数和学习速率，建立一个优化器，并定义几个参数。如果这是一个简单的分类神经网络，我们差不多就完成了。我们只需要设置一个for循环来迭代地运行优化器，以最小化与数据相关的模型错误. 强化学习更复杂，这可能就是你读这本书的原因。我们在前面介绍了主要步骤，但是让我们稍微放大一下。

清单3.3实现了该算法的主循环。概括地说，它的作用如下:

1. 我们设置了一个for循环来记录epochs的数量
2. 在循环中, 我们设置while 循环\(当游戏正在进行时候\)
3. 运行 Q-network前向传播
4. 我们使用 epsilon-greedy，所以在时间t，概率为ε时，我们会选择一个随机动作。在概率为1 - ε的情况下，我们将从我们的神经网络中选择与Q值最高相关的动作。
5. 按照前面的步骤，采取行动a，观察新的状态s '，获得奖励r\_{t+1}。
6. 使用 s' 使网络前向传播, 存储最大的 Q 值, 我们称之为max Q
7. 我们训练网络的目标值为r\_{t} +1 + _γ \*_  max_Q\__{_A_} __\(_S\__{t+1}\)_，_其中γ \(gamma\)是一个介于0和1之间的参数。如果在采取行动 a\_{t} 后游戏结束的话，则没有合法的s\_{t+1}, 所以 _γ \*_  max_Q\__{_A_} __\(_S\__{t+1}\)是无效的，我们可以设置它为 0. 目标变成r\_{t+1}.
8.  鉴于我们有四个输出,我们只需要更新我们采取动作的相关输出.我们的目标输出向量是一样的输出向量从第一次运行,除非我们改变一个和我们动作相关的输出到我们使用Q-learning公式计算出的结果.
9. 在这个样本上训练模型。然后重复步骤2-9。
10. 明确地说，当我们第一次运行我们的神经网络并得到像这样的动作值输出时

```text
array([[-0.02812552, -0.04649779, -0.08819015, -0.00723661]])
```

我们一次迭代的目标向量如下所示:

```text
array([[-0.02812552, -0.04649779, 1, -0.00723661]])
```

在这里，我们只是将一个条目更改到我们想要更新的值。

在继续之前，我们还需要在代码中包含另一个细节。的 Gridworld game engine的makeMove方法使用字符 'u' 进行移动，但我们的Q-learning算法只知道如何生成数字，所以我们需要一个从数字键到动作角色的简单映射:

```text
action_set = {
0: 'u',
1: 'd',
2: 'l',
3: 'r',
}
```

好了，让我们开始编写主训练循环。

```text
    gamma = 0.9
    epsilon = 1.0

    epochs = 1000
    losses = []  # 创建一个列表去存储损失的值，这样我们可以对损失趋势进行绘图
    
    # 主训练循环
    for i in range(epochs):
        game = GridWorld(size=4, mode='static')  # 对于每一个epoch， 我们开始一个新的游戏
        
        # 在我们创建了游戏之后，我们提取状态信息并且加入小规模的噪声
        state_ = game.board.render_np().reshape(1, 64) + np.random.rand(1, 64) / 10.0
        
        state1 = torch.from_numpy(state_).float()  # 将numpy数组先转换成 Pytorch 张量，之后再转换成 pytorch 变量
        status = 1  # 使用状态变量来跟踪游戏是否仍在进行中
        
        while status == 1:  # 当这个游戏仍在进行中，玩到游戏结束，然后开始一个新的epoch
            qval = model(state1)  # 运行 Q-网络前向传播来得到对于所有动作的预测 Q 值
            qval_ = qval.data.numpy()
            
            if random.random() < epsilon:  # 使用 epsilon-greedy 贪心算法进行动作选择
                action_ = np.random.randint(0, 4)
            else:
                action_ = np.argmax(qval_)
            
            action = action_set[action_]   # 将数字中动作转换成 Gridworld 中得代表动作得字符
            game.makeMove(action)  # 在使用epsilon-greedy 方法选取动作后，执行该动作
            state2_ = game.board.render_np().reshape(1, 64) + np.random.rand(1, 64) / 10.0
            
            state2 = torch.from_numpy(state2_).float()  # 再采取动作之后，获取游戏的新状态
            reward = game.reward()
            with torch.no_grad():
                newQ = model(state2.reshape(1, 64))
            maxQ = torch.max(newQ)  # 从新的状态中找到最大的预测 Q 值
            
            if reward == -1:  # 计算目标 Q　值
                Y = reward + (gamma * maxQ)
            else:
                Y = reward
            Y = torch.Tensor([Y]).detach()
            X = qval.squeeze()[action_]  # 创建一个qval数组的副本，然后更新与所采取的操作对应的一个元素
            loss = loss_fn(X, Y)
            optimizer.zero_grad()
            loss.backward()
            losses.append(loss.item())
            state1 = state2
            if reward != -1:  # 如果奖励是-1. 则游戏还没有赢，或者已经碎掉游戏，再或者游戏还在进行中
                status = 0
        
        if epsilon > 0.1:  # 值在每一epoch中递减
            epsilon -= (1 / epochs)
```

{% hint style="info" %}
在游戏状态中加入噪声,可以防止"dead neutons", 当我们使用矫正线性函数 Relu 作为我的激活函数时容易发生这种情况. 基本上，因为我们的游戏状态数组中的大多数元素都是0，所以它们与ReLU不兼容，因为ReLU在 0 是不可微的。因此，我们添加了一点干扰，使状态数组中的值都不完全为0。这也可能有助于过度拟合，即一个模型通过记忆数据中的虚假细节来学习在没有学习数据的抽象特征下，最终防止它泛化为新数据。
{% endhint %}

有几件事需要指出，你可能以前没见过。第一个新特性是在计算下一个状态时使用上下文 torch.no\_grad\(\) Q值。每当我们运行带有一些输入的PyTorch模型时，它就会隐式地创建一个计算图。每个PyTorch张量不仅是张量数据的存储，它还记录了为了生成它而执行了哪些计算。通过使用torch.no\_grad\(\)上下文，我们告诉PyTorch不要在上下文内为代码创建计算图;当我们不需要计算图形时，这将节省内存。当我们计算state2的Q值时，我们只是使用它们作为训练的目标。如果我们没有使用torch，我们就不会通过计算图进行反向传播,如果我们没有使用 torhc.no\_grad。我们只希望通过调model\(state1\)时创建的计算图，因为我们希望通过计算图的建立进行反向传播, 因为我们想根据state1来训练参数，而不是state2. 

下面是一个线性模型的简单例子:

```text
>>> m = torch.Tensor([2.0])
>>> m.requires_grad=True
>>> b = torch.Tensor([1.0])
>>> b.requires_grad=True
>>> def linear_model(x,m,b):
>>> y = m @ x + b
>>> return y
>>> y = linear_model(torch.Tensor([4.]), m,b)
>>> y
tensor([9.], grad_fn=<AddBackward0>)
>>> y.grad_fn
<AddBackward0 at 0x128dfb828>
>>> with torch.no_grad():
>>> y = linear_model(torch.Tensor([4]),m,b)
>>> y
tensor([9.])
>>> y.grad_fn
None
```

我们创建可训练的参数 m 和 b, 通过设置他们的requires\_grad 属性为True, 这意味着PyTorch将把这些参数视为计算图中的节点，并存储它们的计算历史。使用m和b创建的任何新的张量，例如本例中的y，也将requires\_grad设置为True，因此也将保留它们的计算历史记录。您可以看到，当我们第一次调用线性模型并打印y时，它为我们提供了一个带有数值结果的张量，还显示了一个属性grad\_fn=&lt;AddBackward0&gt;. 我们也可以通过打印y.grad\_fn直接看到这个属性。这表明这个张量是由加法运算产生的。它被称为向后加法，因为它实际上存储了加法函数的导数。

如果在给定一个输入的情况下调用这个函数，它会返回两个输出，就像加法 的相反，它接受两个输入并返回一个输出.由于我们的加法函数时两个变量的方程, 这里有一个关于第一个输入的偏导数还有一个关于第二个输入的偏导数。y = a + b对m的偏导是\frac{\partial y}{\partial a} = 1和 \frac{\partial y}{\partial b} = 1。或者 如果 y = a\cdot b 则 \frac{\partial y}{\partial a}=b 或者 \frac{\partial y}{\partial b}=a.  这些是求导的基本规则。当我们从一个给定的节点反向传播时，我们需要它返回所有的偏导数，这就是为什么使用 AddBackward0 梯度函数返回两个输出。

我们可以通过调用 y上的向后方法来验证PyTorch确实像预期的那样计算梯度:

```text
>>> y = linear_model(torch.Tensor([4.]), m,b)
>>> y.backward()
>>> m.grad
tensor([4.])
>>> b.grad
tensor([1.])
```

这就是我们在头脑中或纸上计算这些简单偏导数时得到的结果。为了有效地反向传播，PyTorch会跟踪所有的正向计算并存储它们的导数，以便最终当我们在计算图的输出节点上调用backward\(\)方法时，它会一个节点一个节点地反向通过这些梯度函数，直到输入节点。这就是我们如何得到模型中所有参数的梯度。

注意，我们还对Y张量调用了detach\(\)方法。这实际上是不必要的，因为我们在计算newQ时使用了torch.no\_grad\(\)，但是我们包含了它，因为从计算图中分离节点将在本书的其余部分中变得无处不在，而在训练模型时没有正确地分离节点是常见的bug来源。如果我们称loss.backward\(X,Y\)，并且Y与它自己的具有可训练参数的计算图相关联，我们将反向传播到Y和X，训练过程将学习通过更新X图和Y图中的可训练参数来最小化损失，而我们只想更新X图。我们将Y节点从图中分离出来，这样它就只是作为数据使用，而不是作为计算图节点。您不需要太仔细地考虑细节，但是您确实需要注意您实际上是在向图的哪些部分反向传播，并确保没有向错误的节点反向传播

你可以继续练习循环——1000次就足够了。一旦完成，您就可以绘制损失图，看看训练是否成功，模型是否收敛。这种损失应该在训练时间内或多或少地减少并趋于稳定。我们的plot如图3.10所示。

损失图是相当嘈杂的，但是图的移动平均显著地趋向于零。这给了我们一些信心，我们的训练有效，但我们永远不会知道，直到我们测试它。我们在清单3.4中编写了一个简单的函数，它允许我们在单个游戏上测试模型。

![&#x56FE;3.1 &#x6211;&#x4EEC;&#x7684;&#x7B2C;&#x4E00;&#x4E2A;q&#x5B66;&#x4E60;&#x7B97;&#x6CD5;&#x7684;&#x635F;&#x5931;&#x56FE;&#xFF0C;&#x5B83;&#x5728;&#x8BAD;&#x7EC3;&#x65F6;&#x671F;&#x660E;&#x663E;&#x4E0B;&#x964D;&#x8D8B;&#x52BF;&#x3002;](../../.gitbook/assets/image%20%2890%29.png)

```text
def testModel(model, mode='static', display=True):
    epsilon = 0.05
    max_moves = 50
    win = False
    i = 0
    test_game = GridWorld(mode=mode)
    state_ = test_game.board.render_np().reshape(1, 64) + np.random.rand(1, 64)/10.0
    state = Variable(torch.from_numpy(state_).float())
    if display:
        print("Initial State:")
        print(test_game.display())
    status = 1
    # while game still in progress
    while status == 1:
        qval = model(state)
        qval_ = qval.data.numpy()
        if random.random() < epsilon:
            action_ = np.random.randint(0,4)
        else:
            action_ = np.argmax(qval_)
        # action_ = np.argmax(qval_)  # take action with highest Q-value
        action = action_set[action_]
        if display: print('Move #: %s; Taking action: %s' % (i, action))
        test_game.makeMove(action)
        state_ = test_game.board.render_np().reshape(1, 64) + np.random.rand(1, 64)/10.0
        
        state = Variable(torch.from_numpy(state_).float())
        
        if display:
            print(test_game.display())
        reward = test_game.reward()
        
        if reward == 10:
            status = 0
            win = True
            if display: print("You won! Reward: {}".format(reward,))
        elif reward == -10:
            status = 0
            if display:
                print("Game lost; stepped into the pit. Penalty: {}".format(reward,))
        i += 1  # If we're taking more than 10 actions, just stop, we probably can't win this game
        if i > max_moves:
            if display:
                print("Game lost; too many moves.")
            break
    return win
```

测试函数本质上与我们的训练循环中的代码相同，只是我们不做任何损失计算或反向传播。我们只是向前运行网络来获得预测。让我们看看它是否学会了玩网格世界!

```text
In[30] test_model(model)

Initial State:
[['+' '-' ' ' 'P']
 [' ' 'W' ' ' ' ']
 [' ' ' ' ' ' ' ']
 [' ' ' ' ' ' ' ']]
Move #: 0; Taking action: d
[['+' '-' ' ' ' ']
 [' ' 'W' ' ' 'P']
 [' ' ' ' ' ' ' ']
 [' ' ' ' ' ' ' ']]
Move #: 1; Taking action: l
[['+' '-' ' ' ' ']
 [' ' 'W' 'P' ' ']
 [' ' ' ' ' ' ' ']
 [' ' ' ' ' ' ' ']]
Move #: 2; Taking action: d
[['+' '-' ' ' ' ']
 [' ' 'W' ' ' ' ']
 [' ' ' ' 'P' ' ']
 [' ' ' ' ' ' ' ']]
Move #: 3; Taking action: l
[['+' '-' ' ' ' ']
 [' ' 'W' ' ' ' ']
 [' ' 'P' ' ' ' ']
 [' ' ' ' ' ' ' ']]
Move #: 4; Taking action: l
[['+' '-' ' ' ' ']
 [' ' 'W' ' ' ' ']
 ['P' ' ' ' ' ' ']
 [' ' ' ' ' ' ' ']]
Move #: 5; Taking action: u
[['+' '-' ' ' ' ']
 ['P' 'W' ' ' ' ']
 [' ' ' ' ' ' ' ']
 [' ' ' ' ' ' ' ']]
Move #: 6; Taking action: u
[['+' '-' ' ' ' ']
 [' ' 'W' ' ' ' ']
 [' ' ' ' ' ' ' ']
 [' ' ' ' ' ' ' ']]
Game won! Reward: 10
True
```

请为我们的“Gridworld”玩家鼓掌?很明显，它知道自己在做什么;它直接冲进了球门! 但是我们不要太激动;这是游戏的静态版本，非常简单。如果你使用我们的测试函数mode='random'，你会发现一些令人失望的地方:

```text
>>> testModel(model, 'random')
Initial State:
[[' ' '+' ' ' 'P']
[' ' 'W' ' ' ' ']
[' ' ' ' ' ' ' ']
[' ' ' ' '-' ' ']]
Move #: 0; Taking action: d
[[' ' '+' ' ' ' ']
[' ' 'W' ' ' 'P']
[' ' ' ' ' ' ' ']
[' ' ' ' '-' ' ']]
Move #: 1; Taking action: d
[[' ' '+' ' ' ' ']
[' ' 'W' ' ' ' ']
[' ' ' ' ' ' 'P']
[' ' ' ' '-' ' ']]
Move #: 2; Taking action: l
[[' ' '+' ' ' ' ']
[' ' 'W' ' ' ' ']
[' ' ' ' 'P' ' ']
[' ' ' ' '-' ' ']]
Move #: 3; Taking action: l
[[' ' '+' ' ' ' ']
[' ' 'W' ' ' ' ']
[' ' 'P' ' ' ' ']
[' ' ' ' '-' ' ']]
Move #: 4; Taking action: l
[[' ' '+' ' ' ' ']
[' ' 'W' ' ' ' ']
['P' ' ' ' ' ' ']
[' ' ' ' '-' ' ']]
Move #: 5; Taking action: u
[[' ' '+' ' ' ' ']
['P' 'W' ' ' ' ']
[' ' ' ' ' ' ' ']
[' ' ' ' '-' ' ']]
Move #: 6; Taking action: u
[['P' '+' ' ' ' ']
[' ' 'W' ' ' ' ']
[' ' ' ' ' ' ' ']
[' ' ' ' '-' ' ']]
Move #: 7; Taking action: d
[[' ' '+' ' ' ' ']
['P' 'W' ' ' ' ']
[' ' ' ' ' ' ' ']
[' ' ' ' '-' ' ']]
# we omitted the last several moves to save space
Game lost; too many moves.
```

这真的很有趣。仔细看看网络正在进行的行动。玩家开始游戏时，距离球门只有两张牌。如果它真的知道如何玩游戏，它会走最短的路径达到目标。相反地，它开始向下或向左移动，就像在静态游戏模式中一样。看起来这个模型只是把它训练过的那个板子记了下来，并没有一般化。

也许我们只需要训练它，将游戏模式设置为随机，然后它就会真正学习了?试一试。用随机模式重新训练它。也许你会比我们幸运，但图3.11显示了我们的损失图与随机模式和1000个epoches。

没有迹象表明任何重要的学习是在随机模式下发生的。\(我们不会展示这些结果，但该模型似乎确实学会了如何玩“玩家”模式，即只有玩家被随机放置在网格上。\) 这是一个大问题。如果强化学习所能做的只是学习如何记忆或弱学习，那么它就没有任何价值。但这是DeepMind团队面临的一个问题，他们也解决了这个问题。





