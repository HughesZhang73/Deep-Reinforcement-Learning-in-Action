# 3.2.7-使用神经网络表示Q方程

我们构建一个神经网络作为Q函数。如你所知，在这本书中，我们使用PyTorch来实现我们所有的深度学习模型，但如果你更喜欢另一个框架，如TensorFlow或MXNet，移植这些模型应该是相当简单的。 

图3.8显示了我们将要构建的模型的总体架构。图3.9以弦图的形式展示该网络以及在弦上的数据类型

![&#x56FE;3.8](../../.gitbook/assets/image%20%2876%29.png)

{% hint style="info" %}
我们将使用神经网络模型来玩网格世界。该模型有一个输入层，可以接受64个长度的游戏状态向量，一些隐藏层\(我们使用一个，但两个是通用的\)，以及一个输出层，它为每个动作生成一个4个长度的Q值向量，给定状态。
{% endhint %}

![&#x56FE;3.9](../../.gitbook/assets/image%20%2875%29.png)

{% hint style="info" %}
DQN的弦图。输入是一个64长度的布尔向量，输出是一个由Q值组成的4长度的实向量。
{% endhint %}

为了使用pytorch实现该网络,我们使用 nn 模块, 该模块式Pytorch 高级接口,类似于Tensorflow中的Keras

```text
    # Neural network Q function
    
    l1 = 64
    l2 = 150
    l3 = 100
    l4 = 4
    
    model = torch.nn.Sequential(
        torch.nn.Linear(l1, l2),
        torch.nn.ReLU(),
        torch.nn.Linear(l2, l3),
        torch.nn.ReLU(),
        torch.nn.Linear(l3, l4)
    )
   
    loss_fn = torch.nn.MSELoss()
    learing_rate = 1e-3
    optimizer = torch.optim.Adam(model.parameters(), lr=learing_rate)
    
    gamma = 0.9
    epsilon = 1.0
```

到目前为止，我们所做的就是建立神经网络模型，定义一个损失函数和学习速率，建立一个优化器，并定义几个参数。如果这是一个简单的分类神经网络，我们差不多就完成了。我们只需要设置一个for循环来迭代地运行优化器，以最小化与数据相关的模型错误. 强化学习更复杂，这可能就是你读这本书的原因。我们在前面介绍了主要步骤，但是让我们稍微放大一下。

清单3.3实现了该算法的主循环。概括地说，它的作用如下:

1. 我们设置了一个for循环来记录epochs的数量
2. 在循环中, 我们设置while 循环\(当游戏正在进行时候\)
3. 运行 Q-network前向传播
4. 我们使用 epsilon-greedy，所以在时间t，概率为ε时，我们会选择一个随机动作。在概率为1 - ε的情况下，我们将从我们的神经网络中选择与Q值最高相关的动作。
5. 按照前面的步骤，采取行动a，观察新的状态s '，获得奖励r\_{t+1}。
6. 使用 s' 使网络前向传播, 存储最大的 Q 值, 我们称之为max Q
7. 我们训练网络的目标值为r\_{t} +1 + _γ \*_  max_Q\__{_A_} __\(_S\__{t+1}\)_，_其中γ \(gamma\)是一个介于0和1之间的参数。如果在采取行动 a\_{t} 后游戏结束的话，则没有合法的s\_{t+1}, 所以 _γ \*_  max_Q\__{_A_} __\(_S\__{t+1}\)是无效的，我们可以设置它为 0. 目标变成r\_{t+1}.
8.  鉴于我们有四个输出,我们只需要更新我们采取动作的相关输出.我们的目标输出向量是一样的输出向量从第一次运行,除非我们改变一个和我们动作相关的输出到我们使用Q-learning公式计算出的结果.
9. 在这个样本上训练模型。然后重复步骤2-9。
10. 明确地说，当我们第一次运行我们的神经网络并得到像这样的动作值输出时

```text
array([[-0.02812552, -0.04649779, -0.08819015, -0.00723661]])
```

我们一次迭代的目标向量如下所示:

```text
array([[-0.02812552, -0.04649779, 1, -0.00723661]])
```

在这里，我们只是将一个条目更改到我们想要更新的值。

在继续之前，我们还需要在代码中包含另一个细节。的 Gridworld game engine的makeMove方法使用字符 'u' 进行移动，但我们的Q-learning算法只知道如何生成数字，所以我们需要一个从数字键到动作角色的简单映射:

```text
action_set = {
0: 'u',
1: 'd',
2: 'l',
3: 'r',
}
```

好了，让我们开始编写主训练循环。

```text

```















