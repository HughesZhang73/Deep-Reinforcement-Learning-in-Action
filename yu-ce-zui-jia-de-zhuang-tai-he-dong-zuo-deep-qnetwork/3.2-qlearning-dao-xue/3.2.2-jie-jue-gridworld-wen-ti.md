# 3.2.2-解决Gridworld问题

现在你已经看到了 Q-learning 的公式。让我们后退一步，把这个公式应用到我们的Gridworld问题上。本章的目标是训练一个神经网络从头开始玩一个简单的Gridworld游戏。代理所能访问的只是棋盘的样子，就像人类玩家一样;该算法没有信息优势。而且，我们从一个未经训练的算法开始，所以字面上来说它对游戏世界一无所知, 它没有关于游戏如何运作的预先信息。我们唯一能提供的就是达到目标的奖励。事实上，我们将能够教会算法从无到有地学习玩游戏，这是非常令人印象深刻的。

不像我们人类生活在一个似乎是连续的时间流中，这个算法生活在一个离散的世界中，所以在每个离散的时间步长都需要发生一些事情。在第1步，算法将“查看”游戏棋盘，并决定采取何种行动。然后游戏板将更新，等等。 现在让我们把这个过程的细节画出来。以下是《Gridworld》游戏的事件顺序。

**1.**我们从某个状态开始游戏，我们称之为S\_{t}。这个状态包括了我们所拥有的关于这个游戏的所有信息。对于我们的Gridworld例子，游戏状态表示为一个4×4×4张量。当我们实现这个算法时，我们将会更详细地讨论这个棋盘的细节。

**2.**我们将S\_{t}数据和一个候选动作输入深度神经网络\(或其他一些有趣的机器学习算法\)，它会预测在那种状态下采取该动作的价值\(见图3.2\)。

![&#x56FE;3.2](../../.gitbook/assets/image%20%2874%29.png)

{% hint style="info" %}
Q函数可以是任何接受状态和操作并返回在给定状态下采取该操作的值\(预期奖励\)的函数。
{% endhint %}

记住，算法并不是预测我们在采取特定行动后会得到什么奖励;它是预测期望值\(期望回报\)，也就是我们在一种状态下采取某项行动然后继续按照我们的 π 策略行事所能得到的长期平均回报。在这种状态下，我们可以对几个\(也许是所有\)可能的操作进行这样的操作。  

**3**. 我们采取一个行动，可能是因为我们的神经网络预测它是最高价值的行动，也可能我们采取的是随机行动。我们将把动作标记为A\_{t}。 我们现在处于一个新的游戏状态，我们称之为S\_{t+1}，我们收到或观察到一个标记为R\_{t+1}的奖励。我们想更新我们的学习算法，以反映我们收到的实际奖励，采取它预测的行动是最好的。也许我们得到了一个负面的奖励，或者是一个非常大的奖励，我们想要提高算法预测的准确性\(见图3.3\)。

**4**.现在我们使用S\_{t+1}作为输入运行算法，并找出算法预测的哪个动作具有最高的值。我们称这个值为Q\(S\_{t+1},a\),明确地说，这是一个单一值，它反映了给定新状态和所有可能操作的最高预测Q值。

**5**.现在我们有了更新算法参数所需的所有部件。我们将使用一些损失函数\(如均方误差\)执行一次训练迭代，以**最小化**我们算法的

**预测值** 与 **Q\(S\_{t}, A\_{t}\) + α** _****_**\[R\_{t+1} + γ maxQ\(S\_{t +1}, A\) - Q\(S\_{t}, A\_{t}\)\]** 

的目标预测之间的差异。

![](../../.gitbook/assets/image%20%2882%29.png)

![&#x56FE;3.3](../../.gitbook/assets/image%20%2879%29.png)

{% hint style="info" %}
基于网格世界的q学习原理图。Q函数接受一个状态和一个动作，并返回该状态-动作对的预期回报\(值\)。在采取行动后，我们观察奖励，并使用更新公式，我们使用这个观察来更新Q函数，这样它就能做出更好的预测。
{% endhint %}























