# 3.3.1-灾难性遗忘

在上一节中，当我们尝试用随机模式训练我们的模型时，我们遇到的主要问题有一个名字:灾难性遗忘。在在线训练中，基于梯度下降的训练方法是一个非常重要的问题。 在线训练是我们一直在做的事情:我们在玩游戏的时候每次移动后都进行反向传播。

假设我们的算法正在对图3.12的游戏1进行训练\(学习Q值\)。玩家被放置在球场和球门之间，球门在右边，球场在左边。使用贪心策略，玩家随机移动，偶然向右移动几步，击中目标。太棒了!**算法将通过更新其权值以使输出更接近目标值的方式**\(即通过backpropagation\)来尝试了解这个状态-动作对与一个高值相关联。

现在游戏2已经初始化，玩家再次处于目标和坑之间，但这次目标在左边，坑在右边。也许对我们的朴素算法来说，这个状态和上一个博弈很相似。因为上一次向右移动会获得积极的奖励，所以玩家会选择再次向右移动，但这一次它最终会进入坑中并获得-1奖励。玩家会想:“这是怎么回事?根据我以前的经验，我认为选择右转是最好的决定。”它可以再次反向传播来更新它的状态-动作值，但是因为这个状态-动作非常类似于上次学习到的状态-动作，所以它可以覆盖之前学习到的权值。

这就是灾难性遗忘（catastrophic forgetting）的本质。在非常相似的状态-行为\(但目标不同\)之间存在一种推拉关系，导致无法正确地学习任何东西。我们在监督学习领域通常不会有这个问题，因为我们做的是随机批学习，我们不会更新我们的权值，直到我们迭代了训练数据的一些随机子集，并计算了批学习的和或平均梯度。这样可以使目标达到平均水平，从而稳定学习。

![&#x56FE;3.12 ](../../.gitbook/assets/image%20%2876%29.png)

{% hint style="info" %}
灾难性遗忘的概念是：当两种游戏状态非常相似但却导致截然不同的结果时，Q函数将会感到“困惑”，无法知道该怎么做。在这个例子中，灾难性的遗忘发生是因为Q函数从游戏中学习 \(向右移动将获得+1奖励. 但在第二场比赛中，看起来很相似，它向右移动后得到-1的奖励。结果，算法忘记了它之前学到的关于游戏1的东西，导致基本上没有任何重要的学习。
{% endhint %}







