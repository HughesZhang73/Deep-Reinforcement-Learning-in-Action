# 3.3.2-经验回放

对于游戏的第一种静态模式（static），我们可能不需要担心灾难性的遗忘，因为目标总是固定的，而且该模型确实成功地学会了如何玩这个游戏。但在随机模式中（random），我们需要考虑这一点，这也是为什么我们需要执行所谓的经验重放（experience replay）。 经验回放基本上给我们在在线学习方案中提供了批量更新。这在实现的时候不是很大的问题。

以下是体验重放的工作原理\(图3.13\):

1. 处于状态s，采取行动a，观察新的状态s\_{t +1}，奖励为r\_{t +1}
2. 将其存储为列表中的元组\(s, a, s\_{t +1}, r\_{t +1}\)。 
3. 继续在这个列表中存储每个经验（experience），直到你将列表填满到特定长度为止\(这取决于你自己的定义\)。 
4. 一旦经验回放内存被填满，随机选择一个子集\(同样，你需要定义子集的大小\)。 
5. 遍历这个子集，计算每个子集的值更新 \(value updates\); 将这些存储在目标数组中\(如Y\)，并在X中存储每个记忆的状态 s。
6. 使用X和Y作为批量训练的小批量（mini-batch）。对于数组已满的后续epoch，只需覆盖您的经验重放内存数组中的旧值。

![&#x56FE; 3.13](../../.gitbook/assets/image%20%2895%29.png)

{% hint style="info" %}
这是经验回放的总体概述，一种缓解在线训练算法的主要问题:灾难性遗忘的方法。这个想法**是通过存储过去的经验，然后使用这些经验的随机子集来更新 Q-network，而不是仅仅使用最新的单一经验**。
{% endhint %}







