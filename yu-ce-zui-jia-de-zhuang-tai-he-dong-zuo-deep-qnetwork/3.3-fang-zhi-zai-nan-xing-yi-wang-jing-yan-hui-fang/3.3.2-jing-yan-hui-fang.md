# 3.3.2-经验回放

对于游戏的第一种静态模式（static），我们可能不需要担心灾难性的遗忘，因为目标总是固定的，而且该模型确实成功地学会了如何玩这个游戏。但在随机模式中（random），我们需要考虑这一点，这也是为什么我们需要执行所谓的经验重放（experience replay）。 经验回放基本上给我们在在线学习方案中提供了批量更新。这在实现的时候不是很大的问题。

以下是经验重放的工作原理\(图3.13\):

1. 处于状态s，采取行动a，观察新的状态s\_{t +1}，奖励为r\_{t +1}
2. 将其存储为列表中的元组\(s, a, s\_{t +1}, r\_{t +1}\)。 
3. 继续在这个列表中存储每个经验（experience），直到你将列表填满到特定长度为止\(这取决于你自己的定义\)。 
4. 一旦经验回放内存被填满，随机选择一个子集\(同样，你需要定义子集的大小\)。 
5. 遍历这个子集，计算每个子集的值更新 \(value updates\); 将这些存储在目标数组中\(如Y\)，并在X中存储每个记忆的状态 s。
6. 使用X和Y作为批量训练的小批量（mini-batch）。对于数组已满的后续epoch，只需覆盖您的经验重放内存数组中的旧值。

![&#x56FE; 3.13](../../.gitbook/assets/image%20%2898%29.png)

{% hint style="info" %}
这是经验回放的总体概述，一种缓解在线训练算法的主要问题:灾难性遗忘的方法。这个想法**是通过存储过去的经验，然后使用这些经验的随机子集来更新 Q-network，而不是仅仅使用最新的单一经验**。
{% endhint %}

因此，除了学习你刚刚采取的行动的价值之外，你还需要使用一些随机的过去的经验样本来进行训练，以防止灾难性的遗忘。 清单3.5显示了与清单3.4相同的训练算法，只是添加了经验重放。记住，这次我们训练的是难度更大的游戏变体，所有棋盘碎片都随机放置在格子上。

```text
from collections import deque # 使用双端队列进行操作
epochs = 5000 # 
losses = []
mem_size = 1000  # 设置整个经验回访记忆列表的大小
batch_size = 200 # 设置mini-batch的大小
replay = deque(maxlen=mem_size) # 使用双端队列创建经验回列表
max_moves = 50 # 设置游戏结束前的最大移动次数
h = 0
sync_freq = 500 #A
j=0
for i in range(epochs):
    game = Gridworld(size=4, mode='random')
    state1_ = game.board.render_np().reshape(1,64) + np.random.rand(1,64)/100.0
    state1 = torch.from_numpy(state1_).float()
    status = 1
    mov = 0
    while(status == 1): 
        j+=1
        mov += 1
        qval = model(state1) # 从输入状态计算Q值来选择一个操作
        qval_ = qval.data.numpy()
        if (random.random() < epsilon): # 使用 epsilon-greedy 策略选择一个动作
            action_ = np.random.randint(0,4)
        else:
            action_ = np.argmax(qval_)
        
        action = action_set[action_]
        game.makeMove(action)
        state2_ = game.board.render_np().reshape(1,64) + np.random.rand(1,64)/100.0
        state2 = torch.from_numpy(state2_).float()
        reward = game.reward()
        done = True if reward > 0 else False
        exp =  (state1, action_, reward, state2, done) # 以元组的形式创建经验：（state，reward，action，next state）
        replay.append(exp) # 向经验回访列表中添加经验
        state1 = state2
        
        if len(replay) > batch_size: # 如果回放列表（replay list）至少与小批处理（mini-batch）大小相同，则开始小批处理训练
            minibatch = random.sample(replay, batch_size) #  随机抽样回放列表的子集
            state1_batch = torch.cat([s1 for (s1,a,r,s2,d) in minibatch]) # 将每个经验的组成部分分离成单独的小批量张量
            action_batch = torch.Tensor([a for (s1,a,r,s2,d) in minibatch])
            reward_batch = torch.Tensor([r for (s1,a,r,s2,d) in minibatch])
            state2_batch = torch.cat([s2 for (s1,a,r,s2,d) in minibatch])
            done_batch = torch.Tensor([d for (s1,a,r,s2,d) in minibatch])
            
            # 重新计算小批（mini-batch）状态的Q值以得到梯度
            Q1 = model(state1_batch) 
            with torch.no_grad():
                Q2 = model(state2_batch) # 计算下一批状态的mini-batch的Q值，但不计算梯度
            
            Y = reward_batch + gamma * ((1-done_batch) * torch.max(Q2,dim=1)[0]) # 计算我们想让DQN学习的目标Q值
            X = Q1.gather(dim=1,index=action_batch.long().unsqueeze(dim=1)).squeeze()
            loss = loss_fn(X, Y.detach())
            print(i, loss.item())
            clear_output(wait=True)
            optimizer.zero_grad()
            loss.backward()
            losses.append(loss.item())
            optimizer.step()
            
            if j % sync_freq == 0: #C
                model.load_state_dict(model.state_dict())
                
        if reward != -1 or mov > max_moves: # 如果游戏已经结束，重置status和移动步数
            status = 0
            mov = 0
        
losses = np.array(losses)

```

```text
plt.figure(figsize=(10,7))
plt.plot(losses)
plt.xlabel("Epochs",fontsize=22)
plt.ylabel("Loss",fontsize=22)
```

![](../../.gitbook/assets/image%20%2875%29.png)

为了存储代理的经验，我们使用了 Python的内置集合库中一个称为双端队列（deque）的数据结构。它基本上是一个列表，您可以设置最大大小，以便如果您尝试追加到列表，如果它已经满了，它将删除列表中的第一个项目，并将新项目添加到列表的末尾。这意味着新的经验会取代旧的体验。经验本身是我们附加到重放双端队列的元组 \(state1, reward, action, state2, done\)。

与经验重放训练的主要区别是，当重放列表满时，我们用小批量（mini-batch）的数据进行训练。我们从回放中随机选择了一些经验。我们将单独的经验组件分离为state1\_batch、reward\_batch、action\_batch和state2\_batch。例如,state1 \_batch的大小为batch\_size × 64，或者在本例中为100 × 64。而reward\_batch只是一个长度为100的整数向量。我们遵循与之前完全在线训练相同的训练公式，但现在我们要处理的是小批量（mini-batch）的。我们通过动作索引对使用张量gather方法处理Q1张量\(100 × 4张量\)子集，这样我们只选择与实际选择的动作相关的Q值，从而得到一个100长度的向量。

注意目标Q值，Y = reward\_batch + gamma\(\(1 - done\_batch\) _\*_ torch.max\(Q2,dim=1\)\[0\]\)，如果游戏完成使用done\_batch设置右边为0。记住，如果游戏在采取行动后结束\(我们称之为终端状态\)，那么就不存在下一个状态可以获取最大Q值，所以目标只是成为奖励r\_{t+1}。done变量是一个布尔值，但我们可以把它当作0或1的整数来计算，所以我们只取1 - done，这样如果done =True，1 - done = 0，右边的项就为0。

这次我们训练了5000个epoch，因为这是一个更困难的游戏，但其他方面 q 网络模型和以前一样。当我们测试这个算法时，它似乎能正确地玩大多数游戏。我们还编写了额外的测试脚本，看看它在1000次游戏中获胜的百分比是多少。

```text
max_games = 1000
wins = 0
for i in range(max_games):
    win = test_model(model, mode='random', display=False)
    if win:
        wins += 1
win_perc = float(wins) / float(max_games)
print("Games played: {0}, # of wins: {1}".format(max_games,wins))
print("Win percentage: {}%".format(100.0*win_perc))


Games played: 1000, # of wins: 904
Win percentage: 90.4%
```

当我们在训练过的模型上运行以上清单的代码时\(训练了5000个时代\)，我们得到了大约90%的准确率。你的准确性可能会稍好或稍差。这当然表明它已经学会了如何玩这个游戏，但如果算法真的知道它在做什么，这并不完全是我们所期望的\(尽管你可能通过更长的训练时间来提高准确性\)。一旦你真正知道如何玩游戏，你就应该能够赢得每一场游戏。

需要注意的是，有些初始化的游戏实际上不可能获胜，所以获胜百分比可能永远达不到100%;没有任何逻辑可以阻止进球被困在角落里，被堵在墙和坑后面，导致比赛无法获胜。Gridworld游戏引擎确实阻止了大多数不可能的棋盘配置，但一小部分仍然可以通过。这不仅意味着我们不可能赢得每一场比赛，还意味着学习将会受到轻微的破坏，因为它将尝试遵循一种通常会奏效但无法取胜游戏的策略。我们想要保持游戏逻辑的简单性，以便专注于阐述概念，这样我们就不会使用确保100%获胜的复杂逻辑进行编程。

还有一个原因是我们不能达到95%以上的准确率。让我们看看我们的损失图，如图3.14所示，显示了我们的运行平均损失\(你们的可能差异很大\)。

{% hint style="info" %}
以下是书中的结果
{% endhint %}



![&#x56FE;3.14 ](../../.gitbook/assets/image%20%2896%29.png)

{% hint style="info" %}
执行体验重放后的DQN损失图，显示了明显的下降趋势损失，但它仍然非常嘈杂。
{% endhint %}

在图3.14的损失中，你可以看到它确实呈下降趋势，但它看起来相当不稳定。在有监督学习问题中，您可能会对这种类型的图感到有点惊讶，但在裸DRL中，这种情况很常见。经验重放机制通过减少灾难性遗忘有助于训练的稳定，但也有其他相关的不稳定来源。







