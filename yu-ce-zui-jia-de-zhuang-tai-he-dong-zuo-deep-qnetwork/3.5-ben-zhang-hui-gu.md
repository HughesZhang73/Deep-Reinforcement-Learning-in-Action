# 3.5-本章回顾

我们在这一章已经讲了很多，并且我们再一次潜移默化得提出了了很多基础的强化学习概念。我们本可以在一开始就向您提出一大堆学术定义，但我们抵制住了诱惑，决定尽快开始编写代码。让我们回顾一下我们已经完成的工作，并填补一些术语空白。

在本章中，我们介绍了一种叫做Q-learning的RL算法。q - learning - ing本身与深度学习或神经网络无关;它是一个抽象的数学结构。Q-learning指的是通过学习一个叫做Q函数的函数来解决一个控制任务。你给Q函数一个状态\(游戏邦注:如游戏状态\)，它会预测在给定输入状态下你可能采取的所有可能行动的价值，我们将这些值预测称为Q值。你决定如何处理这些Q值。您可能决定采取与最高Q值相对应的操作\(贪婪方法\)，或者您可能选择一个更复杂的选择过程。正如你在第二章中学到的，你必须平衡探索\(尝试新事物\)和剥削\(采取你所知道的最佳行动\)。在本章中，我们使用了标准的epsilon-greedy方法来选择行动，即我们首先采取随机行动进行探索，然后逐步将我们的策略转变为采取最高价值的行动。

