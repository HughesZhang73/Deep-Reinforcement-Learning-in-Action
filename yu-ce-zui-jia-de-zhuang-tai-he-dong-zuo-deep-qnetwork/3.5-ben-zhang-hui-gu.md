# 3.5-本章回顾

我们在这一章已经讲了很多，并且我们再一次潜移默化得提出了了很多基础的强化学习概念。我们本可以在一开始就向您提出一大堆学术定义，但我们抵制住了诱惑，决定尽快开始编写代码。让我们回顾一下我们已经完成的工作，并填补一些术语空白。

在本章中，我们介绍了一种叫做Q-learning的RL算法。q-learning本身与深度学习或神经网络无关;它是一个抽象的数学结构。Q-learning指的是通过学习一个叫做Q函数的函数来解决一个控制任务。你给Q函数一个状态\(如游戏状态\)，它会预测在给定输入状态下你可能采取的所有可能行动的价值，我们将这些值预测称为Q值。你决定如何处理这些Q值。您可能决定采取与最高Q值相对应的操作\(贪婪方法\)，或者您可能选择一个更复杂的选择过程。正如你在第二章中学到的，你必须平衡探索\(尝试新事物\)和剥削\(采取你所知道的最佳行动\)。在本章中，我们使用了标准的epsilon-greedy方法来选择行动，即我们首先采取随机行动进行探索，然后逐步将我们的策略转变为采取最高价值的行动。

Q函数必须从数据中学习。Q函数必须学会如何对状态做出准确的Q值预测。Q函数可以是任何东西——从一个不智能的数据库到一个复杂的深度学习算法。由于深度学习是我们目前最好的学习算法，我们使用神经网络作为我们的Q函数。这意味着“学习Q函数”和用反向传播训练神经网络是一样的。

到目前为止，Q-learning的一个重要概念是，它是一个off-policy算法，而不是on-policy的算法。你已经从上一章了解了什么是策略:**它是一种算法用来在一段时间内最大化奖励的策略**。如果一个人类正在学习玩网格世界，他们可能会采用一种策略，即先寻找所有可能的通向目标的路径，然后选择最短的那条。 另一个策略可能是随机采取行动，直到你达到目标。

像Q-learning这样的off-policy强化学习算法意味着策略的选择不会影响学习准确Q值的能力。事实上，如果我们随机选择行为，我们的Q-网络可以学习精确的Q值;最终，它将经历多次胜利和失败的游戏，并推断出状态和行动的价值。当然，这是非常低效的，但该策略只有在帮助我们用最少的数据进行学习时才有意义。相反，on-policy算法将明确依赖于策略的选择，或者直接针对从数据中学习策略。换句话说，为了训练我们的DQN，我们需要从环境中收集数据\(经验\)，我们可以使用任何策略来完成这一任务，因此DQN是off- policy。相反，on-policy算法学习一个策略，同时使用同一个策略来收集经验，以便训练自己。

到目前为止，我们保存的另一个关键概念是 model-based 算法和model-free 的算法。要理解这一点，我们首先需要理解什么是模型。我们非正式地使用这个术语来指代神经网络，它经常被用来指代任何一种统计模型，其他的是线性模型或贝叶斯图形模型。在另一种语境中，我们可能会说模型是“现实世界”中事物运作的心理或数学表现。“如果我们能够准确地理解某些东西是如何工作的\(即它由什么组成以及这些组件如何相互作用\)，那么我们不仅能够解释我们已经看到的数据，还能够预测我们还未看到的数据。

例如，天气预报员建立了非常复杂的气候模型，将许多相关变量考虑在内，他们不断测量真实世界的数据。他们可以用他们的模型来预测天气，达到一定的精度。有一个众所周知统计的术语:“所有的模型都是错误的，但有些是有用的，”这意味着不可能建立一个100%符合现实的模型;我们总是会遗漏一些数据或关系。尽管如此，许多模型可以反映一个我们感兴趣的系统中足够多的真相，这些对解释和预测是有用的。

如果我们能建立一个算法来弄清楚Gridworld是如何工作的，它就会推断出Gridworld的模型，它就能完美地运行它。 在q学习中，我们给出的q网络是一个numpy张量。该模型不包含关于 Gridworld 的先验模型（priori model），但它还是通过反复试验学会了如何去玩。我们并没有通过Q-net的工作来弄清楚Gridworld是如何工作的;它唯一的工作就是预测预期的回报。因此Q-learning是一种**无模型**算法。

作为算法的人类建筑师，我们也许能够设计出一些我们自己的领域知识作为模型来优化我们的问题。然后我们可以把这个模型提供给一个学习算法，让它找出细节。 这将是一种基于模型的算法。例如，大多数国际象棋算法都是基于模型的;他们知道国际象棋的规则，知道采取特定的走法会得到什么结果。唯一不为人知的部分\(也是我们想让算法弄清楚的\)是怎样的走法序列将赢得游戏。在有模型的情况下，该算法可以制定长期的计划，以实现其目标。

在许多情况下，我们希望采用的算法可以从无模型发展到有模型规划。例如,机器人学习如何走路可能开始学习通过试验和错误\(模范自由\),但一旦发现基本的走路,它可以推断出其所处环境的模型,然后计划一系列步骤从a点到B\(模型\)。在本书的其余部分，我们将继续探索基于策略的、非策略的、基于模型的和无模型的算法。在下一章中，我们将研究一种算法，它将帮助我们构建一个可以近似策略函数的网络。

### 总结

* 状态空间_state-space_是环境可能处于的所有可能状态的集合。通常状态被编码为张量，所以状态空间可能是一个类型为**R**\_{n}的向量或者是一个矩阵**R**\_{n\*m} 
* 一个动作空间 _action-space_ 是给定一个状态的所有可能动作的集合;例如，象棋游戏的动作空间将是基于某些游戏状态的所有合法步法的集合。
* 一个状态价值_state-value_是在我们遵循某种政策的情况下，对一个状态的折扣奖励的期望总和。如果一个状态具有较高的状态价值，那就意味着从这个状态出发很可能会带来较高的回报。
* 行动价值_action-value_是在特定状态下采取行动的预期回报。它是状态-操作对的值。如果你知道一个状态的所有可能动作的动作价值，你就可以决定采取具有最高动作价值的动作，并且你期望得到最高的奖励。
* 策略函数_policy function_是将状态映射到动作的函数。它是一个函数 “决定”在给定输入状态的情况下采取哪些操作。
* Q函数 _Q function_是一个接受状态-动作对并返回动作-值的函数。
* _Q-learning_ 是一种强化学习，我们试图对Q函数建模;换句话说，我们试图学习如何预测给定状态下每个行动的预期回报。
* _deep Q-network_ \(DQN\)就是我们使用深度学习算法作为 q-learning 模型的地方。
* _off-policy_ 学习是指我们在使用不同策略收集数据时学习策略。
* _on-policy_ 是指我们在学习一个策略的同时，还使用它来收集学习的数据。
* 灾难性遗忘_Catastrophic forgetting_ 是机器学习算法每次用小批数据进行训练时面临的一个大问题，学习的新数据会抹去或破坏已经学习的旧信息。
* 经验回放是一种允许对强化学习算法进行批量训练的机制，以减轻灾难性遗忘并允许稳定的训练。
* 目标网络_target network_是主DQN的副本，我们使用它来稳定用于训练主DQN的更新规则。

