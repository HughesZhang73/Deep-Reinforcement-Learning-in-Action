# 3.4.1-学习稳定性

DeepMind在发表deep Q-network论文时发现的一个潜在问题是，如果你在每次移动之后不断更新Q-network的参数，可能会导致不稳定。**因为奖励可能很少\(注意:我们只会在游戏获胜或失败时给予重要奖励\)，所以每一步都需要更新，而大多数步骤都不会获得任何重要奖励，这可能会导致算法开始不稳定地运行。**

例如，q -网络可能预测在某些状态下“向上”动作的高值;如果它向上移动并且偶然地落在目标上并且获胜，我们将更新Q-network以反映它获得+10奖励的事实。但在另一款游戏中，它认为“向上”是一个非常棒的移动，并预测了一个高Q值，但随后它向上移动并获得-10奖励，所以我们进行了更新，现在它认为“向上”并不那么好。然后，几场比赛之后，**向上移动再次赢得比赛。你可以看到这可能会导致一种振荡行为，在这种情况下，预测的Q值永远不会稳定在一个合理的值上，而只是不断地被折腾。这和灾难性遗忘问题很相似**。

这不仅仅是一个理论问题——这是DeepMind在自己的训练中观察到的。他们设计的解决方案是将Q-网络复制成两个副本，每个副本都有自己的模型参数:“常规”Q-网络和一个被称为目标网络的副本\(符号表示为\hat{Q}-网络，读作“Q hat”\)。在任何训练之前，目标网络在开始时与q -网络是相同的，但是它自己的参数在如何更新方面落后于常规q -网络。

让我们以目标网络为例，再次运行事件序列 \(我们将省略经验回放的细节\):

1. **初始化Q-网络参数\(权值\)θ\_{Q}\(读“theta Q”\)。** 
2. **初始化目标网络作为Q网络的副本，但有单独的参数θ\_{T}\(读“θ T”\)**_**，**_**并设置**_**θ\_T = θ**_**\_Q。** 
3. **使用带有Q-network的Q值的epsilon-greedy策略来选择动作a。** 
4. **观察奖励和新的状态r\_{t+1},s\_{t+1}**
5. **如果情节刚刚被终止\(即，游戏是赢或输\)，目标网络的Q值将被设置为r\_{t+1}或 r\_{t+1} + γ \* maxQ\_{θ\_{r}}\(S\_{t+1}\)，否则\(注意目标网络在这里的使用\)：**
6. **通过Q网络\(不是目标网络\)反向传播目标网络的Q值。** 
7. **每C次迭代，设θ{T} = θ\_{Q}\(即设目标网络的参数等于Q网络的参数\)。**

从图3.15中可以注意到，我们使用目标网络\hat{Q}的唯一一次是计算通过Q-network进行反向传播的目标Q值。我们的想法是在每次训练迭代时更新主要q -网络的参数，但是我们减少了最近的更新对动作选择的影响，希望提高稳定性。

![&#x56FE;3.15](../../.gitbook/assets/image%20%2883%29.png)

{% hint style="info" %}
这是目标网络Q-learning的总体概述。它是普通q学习算法的一个相当直接的扩展，除了你还有一个 Q-网络称为目标网络，它的预测Q值用于向主Q网络反向传播和训练。不训练目标网络的参数，但定期与q网络的参数同步。其思路是利用目标网络的Q值来训练Q-网络，提高训练的稳定性。
{% endhint %}

代码现在变得有点长了，既有经验回放，也有目标网络，所以我们在本书中只看完整代码的一部分。我们将把它留给你去查看这本书的GitHub存储库，在那里你可以找到本章的所有代码。 下面的代码与清单3.5相同，只是添加了目标网络功能的几行代码不同。



```text
# 定义目标网络
import copy

l1 = 64
l2 = 150
l3 = 100
l4 = 4


model = torch.nn.Sequential(
    torch.nn.Linear(l1, l2),
    torch.nn.ReLU(),
    torch.nn.Linear(l2, l3),
    torch.nn.ReLU(),
    torch.nn.Linear(l3,l4)
)

model2 = copy.deepcopy(model) #A 通过复制原始Q-network模型来创建第二个模型
model2.load_state_dict(model.state_dict()) #B 复制原始模型的参数

sync_freq = 50  # 同步频率参数;每隔50个步骤，我们将把model的参数复制到model2中

loss_fn = torch.nn.MSELoss()
learning_rate = 1e-3
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

gamma = 0.9
epsilon = 0.3
```

目标网络只是主DQN的滞后 \(lagged copy\) 拷贝。每个PyTorch模型都有一个state\_dict\(\)方法，它返回组织在字典中的所有参数。 我们使用Python的内置复制模块来复制PyTorch模型数据结构，然后在model2上使用load\_state\_dict方法来确保它已经复制了主DQN的参数。

接下来，我们将包含完整的训练循环，除了在计算下一个状态的最大Q值时使用model2之外，它与清单3.5基本相同。 我们还包含了几行代码，以便每50次迭代将主要模型的参数复制到model2中。

```text
from collections import deque
epochs = 5000
losses = []
mem_size = 1000
batch_size = 200
replay = deque(maxlen=mem_size)
max_moves = 50
h = 0
sync_freq = 500 #A
j=0
for i in range(epochs):
    game = Gridworld(size=4, mode='random')
    state1_ = game.board.render_np().reshape(1,64) + np.random.rand(1,64)/100.0
    state1 = torch.from_numpy(state1_).float()
    status = 1
    mov = 0
    while(status == 1): 
        j+=1
        mov += 1
        qval = model(state1)
        qval_ = qval.data.numpy()
        if (random.random() < epsilon):
            action_ = np.random.randint(0,4)
        else:
            action_ = np.argmax(qval_)
        
        action = action_set[action_]
        game.makeMove(action)
        state2_ = game.board.render_np().reshape(1,64) + np.random.rand(1,64)/100.0
        state2 = torch.from_numpy(state2_).float()
        reward = game.reward()
        done = True if reward > 0 else False
        exp =  (state1, action_, reward, state2, done)
        replay.append(exp) #H
        state1 = state2
        
        if len(replay) > batch_size:
            minibatch = random.sample(replay, batch_size)
            state1_batch = torch.cat([s1 for (s1,a,r,s2,d) in minibatch])
            action_batch = torch.Tensor([a for (s1,a,r,s2,d) in minibatch])
            reward_batch = torch.Tensor([r for (s1,a,r,s2,d) in minibatch])
            state2_batch = torch.cat([s2 for (s1,a,r,s2,d) in minibatch])
            done_batch = torch.Tensor([d for (s1,a,r,s2,d) in minibatch])
            Q1 = model(state1_batch) 
            with torch.no_grad():
                Q2 = model2(state2_batch) #B
            
            Y = reward_batch + gamma * ((1-done_batch) * torch.max(Q2,dim=1)[0])
            X = Q1.gather(dim=1,index=action_batch.long().unsqueeze(dim=1)).squeeze()
            loss = loss_fn(X, Y.detach())
            print(i, loss.item())
            clear_output(wait=True)
            optimizer.zero_grad()
            loss.backward()
            losses.append(loss.item())
            optimizer.step()
            
            if j % sync_freq == 0: #C
                model2.load_state_dict(model.state_dict())
        if reward != -1 or mov > max_moves:
            status = 0
            mov = 0
        
losses = np.array(losses)

#A Set the update frequency for synchronizing the target model parameters to the main DQN
#B Use the target network to get the maiximum Q-value for the next state
#C Copy the main model parameters to the target network
```

```text
plt.figure(figsize=(10,7))
plt.plot(losses)
plt.xlabel("Epochs",fontsize=22)
plt.ylabel("Loss",fontsize=22)
```

![&#x56FE;3.16](../../.gitbook/assets/image%20%2894%29.png)

当我们用经验重放绘制目标网络方法的损失\(图 3.16\)，我们仍然得到一个噪声损失图，但它的噪声明显减少，明显下降趋势。您应该尝试使用超参数进行试验，例如体验回放缓冲区大小、批处理大小、目标网络更新频率和学习速率。性能对这些超参数非常敏感。

当我们在1000场比赛中测试训练过的模型时，与没有目标网络的训练相比，我们的胜率提高了3%。我们获得了大约95%的最高精确度，我们认为这可能是考虑到这种环境的局限性\(例如，不可能获胜的状态的可能性\)的最大精确度。我们只训练了5000个时代，每个时代都是一个游戏。可能的博弈配置的数量\(即状态空间的大小\)约为16手执15手执14手执13 = 43,680\(因为在4 × 4网格上有16个代理可能的位置





