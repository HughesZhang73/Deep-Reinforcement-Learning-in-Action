# 3.4.1-学习稳定性

DeepMind在发表deep Q-network论文时发现的一个潜在问题是，如果你在每次移动之后不断更新Q-network的参数，可能会导致不稳定。**因为奖励可能很少\(注意:我们只会在游戏获胜或失败时给予重要奖励\)，所以每一步都需要更新，而大多数步骤都不会获得任何重要奖励，这可能会导致算法开始不稳定地运行。**

例如，q -网络可能预测在某些状态下“向上”动作的高值;如果它向上移动并且偶然地落在目标上并且获胜，我们将更新Q-network以反映它获得+10奖励的事实。但在另一款游戏中，它认为“向上”是一个非常棒的移动，并预测了一个高Q值，但随后它向上移动并获得-10奖励，所以我们进行了更新，现在它认为“向上”并不那么好。然后，几场比赛之后，**向上移动再次赢得比赛。你可以看到这可能会导致一种振荡行为，在这种情况下，预测的Q值永远不会稳定在一个合理的值上，而只是不断地被折腾。这和灾难性遗忘问题很相似**。

这不仅仅是一个理论问题——这是DeepMind在自己的训练中观察到的。他们设计的解决方案是将Q-网络复制成两个副本，每个副本都有自己的模型参数:“常规”Q-网络和一个被称为目标网络的副本\(符号表示为\hat{Q}-网络，读作“Q hat”\)。在任何训练之前，目标网络在开始时与q -网络是相同的，但是它自己的参数在如何更新方面落后于常规q -网络。

让我们以目标网络为例，再次运行事件序列 \(我们将省略经验回放的细节\):

1. **初始化Q-网络参数\(权值\)θ\_{Q}\(读“theta Q”\)。** 
2. **初始化目标网络作为Q网络的副本，但有单独的参数θ**_**{T}\(读“θ T”\)，**_**并设置**_**θ\_T = θ**_**\_Q。** 
3. **使用带有Q-network的Q值的epsilon-greedy策略来选择动作a。** 
4. **观察奖励和新的状态r\_{t+1},s\_{t+1}**
5. **如果情节刚刚被终止\(即，游戏是赢或输\)，目标网络的Q值将被设置为r\_{t+1}或 r\_{t+1} + γ \* maxQ\_{θ\_{r}}\(S\_{t+1}\)，否则\(注意目标网络在这里的使用\)：**
6. **通过Q网络\(不是目标网络\)反向传播目标网络的Q值。** 
7. **每C次迭代，设θ{T} = θ\_{Q}\(即设目标网络的参数等于Q网络的参数\)。**

从图3.15中可以注意到，我们使用目标网络\hat{Q}的唯一一次是计算通过Q-network进行反向传播的目标Q值。我们的想法是在每次训练迭代时更新主要q -网络的参数，但是我们减少了最近的更新对动作选择的影响，希望提高稳定性。



