---
description: 微积分基础知识
---

# 微积分

### 微积分基础

微积分本质上是对微分和积分的研究。在深度学习中，我们只需要使用区分。微分是求一个函数的导数的过程。

我们已经介绍了导数的一个概念:输出区间与输入区间之比。它告诉你输出空间被拉伸或压缩了多少。 重要的是，这些区间是有向的，所以它们可以是负的或正的，因此比率可以是负的或正的。

{% hint style="info" %}
举例：对于方程f\(x\) = x \* x，任意在曲线上选取有一点x，x在距离为 ε 的域中可以表示成 \(_x – ε, x + ε_\)，ε是任意小的值，设x = 3，ε = 0.1; x = 3附近的区间是\(2.9,3.1\)该区间的大小\(表示\)为3.1 - 2.9 = +0.2，该区间映射为f\(2.9\) = 8.41和f\(3.1\) = 9.61。这个输出区间是\(8.41,9.61\)，其大小是9.61 - 8.41 = 1.2。可见，输出区间仍然是正的，因此比值，即函数f在x = 3处的导数。
{% endhint %}

我们把函数f关于输入变量x的导数记作df/dx，但这不能被认为是一个字面分数;它只是一个符号。我们不需要在点的两边取区间;在一边的区间只要很小就行，例如，我们可以定义一个区间为\(x,x +ε\)，区间的大小是ε，而输出区间的大小是f\(x +ε\)- f\(x\)。

像我们这样使用具体的值只能产生一般的近似;为了得到绝对值，我们需要使用无限小的区间。我们可以这样做，假设ε是一个无限小的数，它大于0，但小于数制中任何其他的数。现在微分变成了一个代数问题。

![&#x5FAE;&#x5206;&#x8F6C;&#x6362;&#x6210;&#x4EE3;&#x6570;&#x95EE;&#x9898;](../../.gitbook/assets/image%20%2811%29.png)

这里我们简单地取输出区间与输入区间之比，两者都是无限小的，因为ε是一个无限小的数字。我们可以代数化简表达式为2x +ε，由于ε是无穷小的，2x +ε无限接近 2x，我们把它作为原函数f\(x\)的真实导数= x2。记住，我们求的是有向区间的比值它可以是正的也可以是负的。我们不仅想知道函数对输入的拉伸\(或挤压\)有多大，还想知道它是否改变了间隔的方向。具体的证明有高等数学的理论，有兴趣的可以研究**非标准分析或光滑无穷小分析。**

### 微积分重要性

微积分的重要性不言而预，在机器学习中，我们试图优化一个函数，这意味着**找到该函数的输入点，使得该函数的输出在所有可能的输入中是最大或最小的**。也就是说，给定一个函数f\(x\)，**我们想找到一个x使f\(x\)小于其它任意x的对应的函数值;我们通常记作argmin\(f\(x\)\)，通常我们有一个损失函数\(或成本或误差函数\)，它接受一些输入向量（input vector）、一个目标向量（target vector）和一个参数向量（parameter vector），并返回预先设定的输出和真实输出之间的误差程度，我们的目标是找到一组参数使误差函数最小化**。有许多可能的方法来最小化这个函数，并不是所有的方法都依赖于使用导数，但在大多数情况下，机器学习中优化损失函数最有效和高效的方法是使用导数。

由于深度学习模型是非线性的\(即，它们不保留加法和标量乘法\)，导数不像线性变换中那样是常量。 从输入点到输出点的挤压或拉伸的数量和方向因点而异。从另一种意义上说，它告诉我们函数的弯曲方向，所以我们可以沿着曲线向下到达最低点。多变量方程例如深度学习模型不只是一个单导数，而是一组偏导数，这些偏导数描述了函数相对于每个输入分量的曲率。通过这种方式，我们可以计算出深度神经网络的哪些参数会导致最小的误差。

用导数信息最小化函数最简单的例子是看看它是如何作用于一个简单的复合函数的。例如我们要找的最小值的函数是：

![](../../.gitbook/assets/image%20%2817%29.png)

如下图所示，可见函数的最小值是在 -1 附近，它是一个复合函数，因为他是具有多项式的复合函数，所以我们需要用微积分中的链式法则来计算导数，这个函数只有一个最低点（valley），所以 它只有一个最小值。

可是深度学习中的模型函数通常是高维的，高复合的（highly compositional），所以会有很多的极小值。理想情况下，我们希望找到全局最小值，也就是函数的最低点。全局极小值或局部极小值是函数上斜率\(即导数\)为0的点。对于一些函数，像这个简单的例子，我们可以用代数解析地计算最小值。深度学习模型通常对于代数计算来说过于复杂，我们必须使用迭代技术。

![](../../.gitbook/assets/image%20%2812%29.png)

微积分中的链式法则为我们提供了一种计算复合函数导数的方法，方法是把复合函数分解成几部分。如果你听说过反向传播（backpropagation），基本上就是把链式法则应用到神经网络上用一些技巧使它更有效。对于我们的例子，让我们重写前面的函数为两个函数:

![](../../.gitbook/assets/image%20%2820%29.png)

我们首先计算外部函数的导数，外部函数是f\(x\) = log\(h\(x\)\)但它只给出df/dh我们真正想要的是df/dx。你可能学过自然对数的导数是：

![](../../.gitbook/assets/image%20%2816%29.png)

内部函数h\(x\)的导数是：

![](../../.gitbook/assets/image%20%2821%29.png)

为了得到复合函数的完整导数，我们注意到这点：

![](../../.gitbook/assets/image%20%2818%29.png)

也就是说，我们想要的导数，df/dx，是由外部函数对输入的导数乘以内部函数\(多项式\)对x的导数得到的。

![](../../.gitbook/assets/image%20%2826%29.png)

### 梯度下降

你可以令导数为0来计算最小值:4x 2 + 3x = 0。这个函数在x = 0和x = -3/4 = -0.75处有两个最小值。但只有x = -0.75是全局最小值，因为f\(-0.75\) = 0.638971，而f\(0\) = 0.693147稍微大一些。

我们来看看如何用梯度下降法来解决这个问题，梯度下降法是一种迭代算法，可以找到函数的最小值。我们从一个随机的x开始。然后计算函数在这一点处的导数，它告诉我们这一点处曲率的大小和方向。然后，我们根据原来的x点、它的导数和步长参数选择一个新的x点，以控制我们移动的速度。也就是说,

![](../../.gitbook/assets/image%20%2823%29.png)

实际代码如下：

```text
# coding=utf-8
import numpy as np


# 原始的复合函数
def f(x):
    return np.log(np.power(x, 4) + np.power(x, 3) + 2)


# 对 x 求导后的方程
def dfdx(x):
    return (4 * np.power(x, 3) + 3 * np.power(x, 2)) / f(x)


# 随机的 x
x = -9.41

# 学习率 Learning Rate （步长）x
lr = 0.001

# 优化迭代的次数
epochs = 5000

for i in range(epochs):
    # 计算当前点 x 的导数
    derive = dfdx(x)
    # 更新当前的 x 点
    x = x - lr * derive
    print(x)
```

我们在训练深度神经网络时也会使用梯度下降的方法，但是深度神经网络是多变量组成函数，所以我们使用偏导数。偏导数并不比普通导数复杂。

考虑多变量函数:

$$
f(x) = x^{4} + y^{2}
$$

这个函数不再有一个单导数，因为它有两个输入变量。我们可以对x或y求导，或者同时对x或y求导。我们可以对x或y求导，或者同时对x或y求导。当我们对一个多变量函数对其所有输入函数求导并将其打包成一个向量时，我们称之为梯度（gradient），使用nabla符号 ∇ 表示。例如：_∇f\(x\) =_ \[_df/dx,df/dy_\]。所以为了计算f关于x的偏导数，也就是df/dx，我们简单地把另一个变量y设为常数，然后像往常一样微分。这里df/dx = 4\*x^{3} df/dy = 2y。所以梯度∇f\(x\) = \[4\*x^{3}, 2y\]，这是偏导数的向量。然后我们可以像往常一样运行梯度下降，现在我们在深度神经网络的误差函数中找到与最低点相关的向量。









